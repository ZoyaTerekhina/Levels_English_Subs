{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85dd915c",
   "metadata": {},
   "source": [
    "**ОПРЕДЕЛЕНИЕ УРОВНЯ СЛОЖНОСТИ АНГЛОЯЗЫЧНЫХ ФИЛЬМОВ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6c0f0",
   "metadata": {},
   "source": [
    "Задача: разработать модель машинного обучения, которая позволит по анлаизу субтитров оценить уровень сложности англоязычного фильма"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba521375",
   "metadata": {},
   "source": [
    "Заказчиком предоставлены следующие материалы:\n",
    "  - Файлы с субтитрами англоязычных фильмов, рассортированные по уровням сложности, согласно шкале CEFR: A1, A2, B1, B2, C1, C2. Каждый файл имеет формат .srt\n",
    "  - Файл в формате excel, сдержащий наименование англоязычных фильмов с уже известным уровнем сложности.\n",
    "  - Словари: Американский на 3000 слов и на 5000 слов, Оксфордский на 3000 слов и на 5000 слов в формате pdf. В данных словарях для каждого уровня сложности из системы CEFR прописан набор слова."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1435c2e",
   "metadata": {},
   "source": [
    "В процессе работы нам необходимо:\n",
    "\n",
    "- провести анализ предоставленного заказчиком материала\n",
    "- подготовить материал для машинного обучения\n",
    "- создать и обучить несколькоих моделей, предсказывающих уровень сложности субтитров\n",
    "- определить оптимальную модель на основании выбранной для анализа метрики\n",
    "- создать приложение на плтаформе streamlit для демонстрации работы модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcf7c70",
   "metadata": {},
   "source": [
    "**ИМПОРТИРУЕМ НЕОБХОДИМЫЕ ДЛЯ РАБОТЫ БИБЛИОТЕКИ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d950d736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysrt in c:\\users\\zst\\anaconda3\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: chardet in c:\\users\\zst\\anaconda3\\lib\\site-packages (from pysrt) (4.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8f6bdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\zst\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from PyPDF2) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "edbf35f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\zst\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\zst\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zst\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\zst\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\zst\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7e8841bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\zst\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (1.4.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (1.7.3)\n",
      "Requirement already satisfied: plotly in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (5.6.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (3.5.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (1.21.5)\n",
      "Requirement already satisfied: six in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n"
     ]
    }
   ],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8739b86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\zst\\anaconda3\\lib\\site-packages (3.3.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\zst\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from lightgbm) (1.2.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\zst\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\zst\\anaconda3\\lib\\site-packages (from lightgbm) (1.21.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5eb95dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pysrt\n",
    "import re\n",
    "import chardet\n",
    "import string\n",
    "import pymorphy2\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "import pickle\n",
    "from pickle import dump,load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None # для игнорирования предупреждения\n",
    "# уберем ограничения на количество выводимых столбцов, что бы просмотреть все столбцы\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from functools import reduce\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbbbd4",
   "metadata": {},
   "source": [
    "**РАЗБЕРЕМ НАБОР ФАЙЛОВ Subtitles_all**\n",
    "______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a7424af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Введем константы для указания путей к файлам\n",
    "PATH_SUBTITLES_ALL = \"./English_level/English_level/English_scores/Subtitles_all\" \n",
    "MOV_LAB_PATH = \"./English_level/English_level/English_scores/movies_labels.xlsx\"\n",
    "DICTIONARY = \"./English_level/English_level/Oxford_CEFR_level/dictionary.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "6ea9051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция очистки субтитров от лишних символов:\n",
    "\n",
    "# Константы:\n",
    "HTML = r'<.*?>' # html тэги меняем на пробел\n",
    "TAG = r'{.*?}' # тэги меняем на пробел\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]' # комменты в скобках меняем на пробел\n",
    "UPPER = r'[[A-Za-z ]+[\\:\\]]' # указания на того кто говорит (BOBBY:)\n",
    "LETTERS = r'[^a-zA-Z\\'.,!? ]' # все что не буквы меняем на пробел \n",
    "SPACES = r'([ ])\\1+' # повторяющиеся пробелы меняем на один пробел\n",
    "DOTS = r'[\\.]+' # многоточие меняем на точку\n",
    "SYMB = r\"[^\\w\\d'\\s]\" # знаки препинания кроме апострофа\n",
    "\n",
    "def clean_subs(sentence):\n",
    "    sentence = re.sub(r'\\n',' ', sentence)\n",
    "    sentence = re.sub(HTML, ' ', sentence) # html тэги меняем на пробел\n",
    "    sentence = re.sub(TAG, ' ', sentence) # тэги меняем на пробел\n",
    "    sentence = re.sub(COMMENTS, ' ', sentence) # комменты в скобках меняем на пробел\n",
    "    sentence = re.sub(UPPER, ' ', sentence) # указания на того кто говорит (BOBBY:)\n",
    "    sentence = re.sub(LETTERS, ' ', sentence) # все что не буквы меняем на пробел\n",
    "    sentence = re.sub(DOTS, r'.', sentence) # многоточие меняем на точку\n",
    "    sentence = re.sub(SPACES, r'\\1', sentence) # повторяющиеся пробелы меняем на один пробел\n",
    "    sentence = re.sub(SYMB, '', sentence) # знаки препинания кроме апострофа на пустую строку\n",
    "    sentence = re.sub('www', '', sentence) # кое-где остаётся www, то же меняем на пустую строку\n",
    "    sentence = re.sub(r'(\\ufeff)?\\d+\\t?\\d{1,2}:\\d{1,2}:\\d{1,2},\\d{1,5}\\t?\\d{1,3}:\\d{1,2}:\\d{1,2},\\d{1,5}\\t?', '', sentence) # Удаление временной метки\n",
    "    sentence = sentence.lstrip() # обрезка пробелов в начале и в конце\n",
    "    sentence = sentence.encode('ascii', 'ignore').decode() # удаляем все что не ascii символы   \n",
    "    sentence = sentence.lower() # текст в нижний регистр     \n",
    "    return sentence\n",
    "# функция чтения субтитров в файле\n",
    "def read_sub(subs):\n",
    "    text = []\n",
    "    for i in subs:\n",
    "        sentence = clean_subs(i.text_without_tags)\n",
    "        text.append(sentence)\n",
    "    return ' '.join(text)\n",
    "# функция чтения файлов из общего каталога\n",
    "def read_file (dirname, filename):\n",
    "    fullpath = os.path.join (dirname, filename)\n",
    "  \n",
    "    try:\n",
    "        enc = chardet.detect(open(fullpath, \"rb\").read())['encoding']\n",
    "        subs = pysrt.open(fullpath, enc)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print('файл не читается', fullpath, filename)\n",
    "        return False\n",
    "    return read_sub(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "819b2ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 279 entries, 0 to 278\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   movie      279 non-null    object\n",
      " 1   subtitels  279 non-null    object\n",
      " 2   level      279 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 6.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# считываем файлы субтитров:\n",
    "files = []\n",
    "title = []\n",
    "level = []\n",
    "for dirpath, _, filenames in os.walk(PATH_SUBTITLES_ALL):   \n",
    "    for file in filenames:\n",
    "        title.append(file.replace('.srt', ''))\n",
    "        level.append(os.path.basename(dirpath))\n",
    "        result = read_file(dirpath, file) \n",
    "        files.append(result)\n",
    "       \n",
    "        \n",
    "# создаем датафрем:\n",
    "data = pd.DataFrame({'movie':title, 'subtitels':files, 'level':level})  \n",
    "\n",
    "#выведем первые 10 строк датафрейма и информацию о датасете:\n",
    "data.head(10)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b018495",
   "metadata": {},
   "source": [
    "***РАЗБЕРЕМ ДОКУМЕНТ Movies_Lables***\n",
    "_________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd4540",
   "metadata": {},
   "source": [
    "Скачаем и переведем в датасет экселевский файл с наименованием фильмов и указанием уровня сложности данного фильма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4f1ea088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>All_dogs_go_to_heaven(1989)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>An_American_tail(1986)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Babe(1995)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Back_to_the_future(1985)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Banking_On_Bitcoin(2016)</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Batman_begins(2005)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Beauty_and_the_beast(2017)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Before_I_go_to_sleep(2014)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Before_sunrise(1995)</td>\n",
       "      <td>B1, B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Before_sunset(2004)</td>\n",
       "      <td>B1, B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                             movie   level\n",
       "0    0         10_Cloverfield_lane(2016)      B1\n",
       "1    1  10_things_I_hate_about_you(1999)      B1\n",
       "2    2              A_knights_tale(2001)      B2\n",
       "3    3              A_star_is_born(2018)      B2\n",
       "4    4                     Aladdin(1992)  A2/A2+\n",
       "5    5       All_dogs_go_to_heaven(1989)  A2/A2+\n",
       "6    6            An_American_tail(1986)  A2/A2+\n",
       "7    7                        Babe(1995)  A2/A2+\n",
       "8    8          Back_to_the_future(1985)  A2/A2+\n",
       "9    9          Banking_On_Bitcoin(2016)      C1\n",
       "10  10               Batman_begins(2005)  A2/A2+\n",
       "11  11        Beauty_and_the_beast(2017)      B2\n",
       "12  12        Before_I_go_to_sleep(2014)      B2\n",
       "13  13              Before_sunrise(1995)  B1, B2\n",
       "14  14               Before_sunset(2004)  B1, B2"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_labels = pd.read_excel(MOV_LAB_PATH)\n",
    "# Откорректируем названия столбцов для удобства дальнейшей работы с ними:\n",
    "movies_labels.columns = movies_labels.columns.str.lower()\n",
    "\n",
    "movies_labels.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ce8c7d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов в наборе данных: 0\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на наличие полных дубликатов:\n",
    "print('Количество дубликатов в наборе данных:', movies_labels.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d7624f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалим колонку id:\n",
    "movies_labels.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "405d131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов в наборе данных: 2\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на наличие дубликатов без колонки id:\n",
    "print('Количество дубликатов в наборе данных:', movies_labels.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d611245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3f52ce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов в наборе данных после удаления: 0\n"
     ]
    }
   ],
   "source": [
    "# Удалим дубликаты:\n",
    "movies_labels.drop_duplicates(inplace=True)\n",
    "print('Количество дубликатов в наборе данных после удаления:', movies_labels.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7e0fc1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2            101\n",
       "B1             53\n",
       "C1             40\n",
       "A2/A2+         26\n",
       "B1, B2          8\n",
       "A2              6\n",
       "A2/A2+, B1      5\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на уникальные значения столбца Level:\n",
    "movies_labels['level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe72d0",
   "metadata": {},
   "source": [
    "Как мы видем, некоторые фильмы имеют двойное или даже тройное обочначение уровня фильма. Заменим такие значения на максимальные по уровню сложности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a69ddc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_labels['level'] = movies_labels['level'].replace('A2/A2+','A2').replace ('B1, B2','B1').replace ('A2/A2+, B1','B1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3d626113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 239 entries, 0 to 240\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   movie   239 non-null    object\n",
      " 1   level   239 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "movies_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "0779b856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All_dogs_go_to_heaven(1989)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An_American_tail(1986)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Babe(1995)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Back_to_the_future(1985)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Banking_On_Bitcoin(2016)</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie level\n",
       "0         10_Cloverfield_lane(2016)    B1\n",
       "1  10_things_I_hate_about_you(1999)    B1\n",
       "2              A_knights_tale(2001)    B2\n",
       "3              A_star_is_born(2018)    B2\n",
       "4                     Aladdin(1992)    A2\n",
       "5       All_dogs_go_to_heaven(1989)    A2\n",
       "6            An_American_tail(1986)    A2\n",
       "7                        Babe(1995)    A2\n",
       "8          Back_to_the_future(1985)    A2\n",
       "9          Banking_On_Bitcoin(2016)    C1"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39114230",
   "metadata": {},
   "source": [
    "**ОБЪЕДИНИМ ДВА ДАТАСЕТА, РАССМОТРЕННЫХ РАНЕЕ В ОДИН:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "322ee6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 396 entries, 0 to 395\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   movie      396 non-null    object\n",
      " 1   subtitels  279 non-null    object\n",
      " 2   level      396 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 12.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dfs = [data, movies_labels]\n",
    "#merge all DataFrames into one\n",
    "final_df = reduce(lambda left,right: pd.merge(left,right,on=['movie','level'],\n",
    " how='outer'), dfs)#.fillna('none')\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a068d5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Suits.Episode 8- Mea Culpa</td>\n",
       "      <td>so we started this thing i was just doing it t...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Suits.Episode 9- Uninvited Guests</td>\n",
       "      <td>pursuant to section b of the bylaws i am placi...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Suits.S01E01.1080p.BluRay.AAC5.1.x265-DTG.02.EN</td>\n",
       "      <td>gerald tate's here he wants to know what's hap...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Suits.S01E02.1080p.BluRay.AAC5.1.x265-DTG.02.EN</td>\n",
       "      <td>uh what is that three in a row that would be f...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Suits.S01E03.1080p.BluRay.AAC5.1.x265-DTG.02.EN</td>\n",
       "      <td>the tesla roadster sport offered only to our m...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                movie  \\\n",
       "0       The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1                The Walking Dead-S01E02-Guts.English   \n",
       "2   The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3               The Walking Dead-S01E04-Vatos.English   \n",
       "4            The Walking Dead-S01E05-Wildfire.English   \n",
       "..                                                ...   \n",
       "85                         Suits.Episode 8- Mea Culpa   \n",
       "86                  Suits.Episode 9- Uninvited Guests   \n",
       "87    Suits.S01E01.1080p.BluRay.AAC5.1.x265-DTG.02.EN   \n",
       "88    Suits.S01E02.1080p.BluRay.AAC5.1.x265-DTG.02.EN   \n",
       "89    Suits.S01E03.1080p.BluRay.AAC5.1.x265-DTG.02.EN   \n",
       "\n",
       "                                            subtitels level  \n",
       "0           little girl i'm a policeman little gir...    A2  \n",
       "1    mom right here  any luck how do we tell if th...    A2  \n",
       "2    that's right you heard me bitch you got a pro...    A2  \n",
       "3    what nothing it's not nothing it's always som...    A2  \n",
       "4   walkie talkie squawks morgan i don't know if y...    A2  \n",
       "..                                                ...   ...  \n",
       "85  so we started this thing i was just doing it t...    B2  \n",
       "86  pursuant to section b of the bylaws i am placi...    B2  \n",
       "87  gerald tate's here he wants to know what's hap...    B2  \n",
       "88  uh what is that three in a row that would be f...    B2  \n",
       "89  the tesla roadster sport offered only to our m...    B2  \n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c560f24e",
   "metadata": {},
   "source": [
    "Обработаем полученные данные в столбцах:\n",
    " - уберем в колонке level значение, не являющее вровнем (Subtitles)\n",
    " - удалим строки с NAN в колонке subtitels\n",
    " - проверем колонку movie на наличие скрытых дубликатов, приведя наименования немного в порядок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "73c8b5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A2', 'B1', 'B2', 'C1', 'Subtitles']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# смотрим уникальные значения колонки level:\n",
    "sorted(final_df['level'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "096ccb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2           141\n",
       "Subtitles    116\n",
       "B1            67\n",
       "C1            40\n",
       "A2            32\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "698c21e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2    141\n",
       "B1     67\n",
       "C1     40\n",
       "A2     32\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удаляем фильмы, уровень которых опеределен как Subtitles\n",
    "final_df = final_df[final_df['level'] != \"Subtitles\"]\n",
    "final_df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b7f8df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 280 entries, 0 to 395\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   movie      280 non-null    object\n",
      " 1   subtitels  163 non-null    object\n",
      " 2   level      280 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 8.8+ KB\n"
     ]
    }
   ],
   "source": [
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "25525651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>Matilda(2022)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Bullet train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Thor: love and thunder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Lightyear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>The Grinch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                movie subtitels level\n",
       "279         10_Cloverfield_lane(2016)       NaN    B1\n",
       "280  10_things_I_hate_about_you(1999)       NaN    B1\n",
       "281              A_knights_tale(2001)       NaN    B2\n",
       "282              A_star_is_born(2018)       NaN    B2\n",
       "283                     Aladdin(1992)       NaN    A2\n",
       "..                                ...       ...   ...\n",
       "391                     Matilda(2022)       NaN    C1\n",
       "392                      Bullet train       NaN    B1\n",
       "393            Thor: love and thunder       NaN    B2\n",
       "394                         Lightyear       NaN    B2\n",
       "395                        The Grinch       NaN    B1\n",
       "\n",
       "[117 rows x 3 columns]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Рассмотрим строки в котрых отсутсвуют субтитры.\n",
    "final_df.loc[final_df['subtitels'].isna() ==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "325b9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163 entries, 0 to 162\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   movie      163 non-null    object\n",
      " 1   subtitels  163 non-null    object\n",
      " 2   level      163 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#удаляем строки с отсутствующими субтитрами и смотрим на оствшийся датасет\n",
    "#final_df = final_df.dropna(axis='index', how='any', subset=['subtitels']).reset_index(drop=True)\n",
    "final_df = final_df.dropna(axis='index', subset=['subtitels']).reset_index(drop=True)\n",
    "final_df.head(20)\n",
    "final_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "374bb714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AmericanBeauty1999.BRRip',\n",
       " \"Angela's.Christmas.2018.WEBRip.Netflix\",\n",
       " 'Angelas.Christmas.Wish.2020',\n",
       " 'Collateral.Beauty.2016.720p.BRRip.x264.AAC-ETRG',\n",
       " 'Crazy4TV.com - Suits.S06E01.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E02.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E03.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E04.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E05.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E07.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E08.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E09.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E10.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E11.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E12.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E13.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E14.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E15.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E16.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crown, The S01E01 - Wolferton Splash.en',\n",
       " 'Crown, The S01E01 - Wolferton Splash.en.SDH',\n",
       " 'Crown, The S01E02 - Hyde Park Corner.en',\n",
       " 'Crown, The S01E02 - Hyde Park Corner.en.SDH',\n",
       " 'Crown, The S01E03 - Windsor.en',\n",
       " 'Crown, The S01E03 - Windsor.en.FORCED',\n",
       " 'Crown, The S01E03 - Windsor.en.SDH',\n",
       " 'Crown, The S01E04 - Act of God.en',\n",
       " 'Crown, The S01E04 - Act of God.en.SDH',\n",
       " 'Crown, The S01E05 - Smoke and Mirrors.en',\n",
       " 'Crown, The S01E05 - Smoke and Mirrors.en.FORCED',\n",
       " 'Crown, The S01E05 - Smoke and Mirrors.en.SDH',\n",
       " 'Crown, The S01E06 - Gelignite.en',\n",
       " 'Crown, The S01E06 - Gelignite.en.SDH',\n",
       " 'Crown, The S01E07 - Scientia Potentia Est.en',\n",
       " 'Crown, The S01E07 - Scientia Potentia Est.en.FORCED',\n",
       " 'Crown, The S01E07 - Scientia Potentia Est.en.SDH',\n",
       " 'Crown, The S01E08 - Pride & Joy.en',\n",
       " 'Crown, The S01E08 - Pride & Joy.en.SDH',\n",
       " 'Crown, The S01E09 - Assassins.en',\n",
       " 'Crown, The S01E09 - Assassins.en.SDH',\n",
       " 'Crown, The S01E10 - Gloriana.en',\n",
       " 'Crown, The S01E10 - Gloriana.en.FORCED',\n",
       " 'Crown, The S01E10 - Gloriana.en.SDH',\n",
       " 'Downton Abbey - S01E01 - Episode 1.eng.SDH',\n",
       " 'Downton Abbey - S01E02 - Episode 2.eng.SDH',\n",
       " 'Downton Abbey - S01E03 - Episode 3.eng.SDH',\n",
       " 'Downton Abbey - S01E04 - Episode 4.eng.SDH',\n",
       " 'Downton Abbey - S01E05 - Episode 5.eng.SDH',\n",
       " 'Downton Abbey - S01E06 - Episode 6.eng.SDH',\n",
       " 'Downton Abbey - S01E07 - Episode 7.eng.SDH',\n",
       " 'Frozen.2013.WEB-DL.DSNP',\n",
       " 'Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x264.YIFY',\n",
       " 'Indiana Jones And The Last Crusade DVDRip Xvid -IZON-',\n",
       " 'Men.In.Black.1997.720p.Bluray.x264-SEPTiC',\n",
       " 'Rat.Race.2001.1080p.WEB-DL.DD5.1.H264-FGT',\n",
       " 'Roman Holiday 1953 1080p WEBRip HEVC AAC',\n",
       " 'SOMM.Into.the.Bottle.2015.1080p.BluRay.x265-RARBG.en',\n",
       " \"Secrets Of Her Majesty's Secret Service eng\",\n",
       " 'Seven.Worlds.One.Planet.S01E01.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E02.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E03.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E04.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E05.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E06.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E07.2160p.BluRay.Remux.eng',\n",
       " 'SlingShot (2014) WEB.eng',\n",
       " 'Spirit.Stallion.of.the.Cimarron.EN',\n",
       " 'Suits S04E01 EngSub',\n",
       " 'Suits S04E02 EngSub',\n",
       " 'Suits S04E03 EngSub',\n",
       " 'Suits S04E04 EngSub',\n",
       " 'Suits S04E05 EngSub',\n",
       " 'Suits S04E06 EngSub',\n",
       " 'Suits S04E07 EngSub',\n",
       " 'Suits S04E08 EngSub',\n",
       " 'Suits S04E09 EngSub',\n",
       " 'Suits S04E10 EngSub',\n",
       " 'Suits S04E11 EngSub',\n",
       " 'Suits S04E12 EngSub',\n",
       " 'Suits S04E13 EngSub',\n",
       " 'Suits S04E14 EngSub',\n",
       " 'Suits S04E15 EngSub',\n",
       " 'Suits S04E16 EngSub',\n",
       " 'Suits.Episode 1- Denial',\n",
       " 'Suits.Episode 10- Faith',\n",
       " 'Suits.Episode 11- Blowback',\n",
       " 'Suits.Episode 12- Live to Fight',\n",
       " \"Suits.Episode 13- God's Green Earth\",\n",
       " 'Suits.Episode 14- Self Defense',\n",
       " 'Suits.Episode 15- Tick Tock',\n",
       " 'Suits.Episode 16- 25th Hour',\n",
       " 'Suits.Episode 2- Compensation',\n",
       " 'Suits.Episode 3- No Refills',\n",
       " 'Suits.Episode 4- No Puedo Hacerlo',\n",
       " 'Suits.Episode 5- Toe to Toe',\n",
       " 'Suits.Episode 6- Privilege',\n",
       " 'Suits.Episode 7- Hitting Home',\n",
       " 'Suits.Episode 8- Mea Culpa',\n",
       " 'Suits.Episode 9- Uninvited Guests',\n",
       " 'Suits.S01E01.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E02.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E03.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E04.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E05.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E06.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E07.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E08.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E09.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E10.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E11.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E12.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S02E01.HDTV.x264-AVS',\n",
       " 'Suits.S02E02.HDTV.x264-ASAP',\n",
       " 'Suits.S02E03.HDTV.x264-ASAP',\n",
       " 'Suits.S02E04.HDTV.x264-ASAP',\n",
       " 'Suits.S02E05.iNTERNAL.HDTV.x264-2HD',\n",
       " 'Suits.S02E06.All.In.HDTV.x264-FQM',\n",
       " 'Suits.S02E07.Sucker.Punch.PROPER.HDTV.x264-FQM',\n",
       " 'Suits.S02E08.HDTV.x264-EVOLVE',\n",
       " 'Suits.S02E09.HDTV.x264-ASAP',\n",
       " 'Suits.S02E10.HDTV.x264-ASAP',\n",
       " 'Suits.S02E11.HDTV.x264-ASAP',\n",
       " 'Suits.S02E12.HDTV.x264-ASAP',\n",
       " 'Suits.S02E13.REPACK.HDTV.x264-2HD',\n",
       " 'Suits.S02E14.HDTV.x264-ASAP',\n",
       " 'Suits.S02E15.HDTV.x264-ASAP',\n",
       " 'Suits.S02E16.HDTV.x264-2HD',\n",
       " 'Suits.S03E01.480pHDTV.x264-mSD',\n",
       " 'Suits.S03E02.720pHDTV.x264-mSD',\n",
       " 'Suits.S03E03.480pHDTV.x264-mSD',\n",
       " 'Suits.S03E04.480pHDTV.x264-mSD',\n",
       " 'Suits.S03E05.480p.HDTV.x264-mSD',\n",
       " 'Suits.S03E06.720p.HDTV.x264-mSD',\n",
       " 'Suits.S03E07.HDTV.x264-mSD',\n",
       " 'Suits.S03E08.480p.HDTV.x264-mSD',\n",
       " 'Suits.S03E09.480p.HDTV.x264-mSD',\n",
       " 'Suits.S03E10.HDTV.x264-mSD',\n",
       " 'Terminator_2_Judgment_Day_1991_roNy',\n",
       " 'The Walking Dead-S01E01-Days Gone Bye.English',\n",
       " 'The Walking Dead-S01E02-Guts.English',\n",
       " 'The Walking Dead-S01E03-Tell It To The Frogs.English',\n",
       " 'The Walking Dead-S01E04-Vatos.English',\n",
       " 'The Walking Dead-S01E05-Wildfire.English',\n",
       " 'The Walking Dead-S01E06-TS-19.English',\n",
       " 'The.Grinch.2018.REMUX.1080p.Blu-ray.AVC.TrueHD.DTS-HD.MA.7.1-LEGi0N.English',\n",
       " 'The.Notebook.DVDRip.XviD-DiAMOND',\n",
       " 'The.Sound.of.Music.1965.WEBRip.iTunes',\n",
       " 'The.True.Cost.2015.BluRay.720p.700MB.Ganool.com',\n",
       " \"Valentine's.Day.2010.Subtitles.YIFY\",\n",
       " 'Virgin.River.S01E01.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'Virgin.River.S01E02.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'Virgin.River.S01E03.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'Virgin.River.S01E04.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'Virgin.River.S01E05.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'Virgin.River.S01E06.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'Virgin.River.S01E07.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'Virgin.River.S01E08.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'Virgin.River.S01E09.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'Virgin.River.S01E10.INTERNAL.720p.WEB.x264-STRiFE',\n",
       " 'icarus.2017.web.x264-strife',\n",
       " 'mechanic-resurrection_',\n",
       " 'z srt23 uk-bun Gullivers.Travels.1939.720p.BluRay.x264-CiNEFiLE']"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# рассмотри колонку с наименование фильмов и слегка приведем ее в порядок при необходимости:\n",
    "sorted(final_df['movie'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "3b275062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов: 0\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на наличие полных дубликатов:\n",
    "print('Количество дубликатов:', final_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a01a6",
   "metadata": {},
   "source": [
    "Полных дубликатов нет, но присутствуют названия, к которым можно было бы предраться и удалить в виду того, что отличаются они скорее всего лишь форматом воспроизведения. Но тогда у нас будет совсем маленький датасет для обучения модели. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80db5b1",
   "metadata": {},
   "source": [
    "**СЛОВАРИ**\n",
    "\n",
    "Заказчиком было предоставлено несколько словарей с разбиением слов по уровнем сложности английского языка.\n",
    "Предварительно словари были переведены из pdf формата в exel И объединены.\n",
    "\n",
    "Считаем полученный словарь и сформируем из него датасет.\n",
    "\n",
    "У нас по два словаря:\n",
    "Американский на 3000 слов и на 5000 слов \n",
    "Оксфордский на 3000 слов и на 5000 слов\n",
    "\n",
    "Сделаем замену наименований словарей на просто USA и Oxford\n",
    "Отсортируем данные по кадому из словарей и найдем дубликаты.\n",
    "Будем оставлять каждый первый (ориентируемся, что если слово есть и в легком и в сложном уровне, то скорее всего студент его уже знает и изучил все семантические значения, как только узнал слово, т.е. с первого уровня.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "338770f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>word</th>\n",
       "      <th>comment</th>\n",
       "      <th>semantic</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>an</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1</td>\n",
       "      <td>about</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>above</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1</td>\n",
       "      <td>across</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  level    word comment            semantic  \\\n",
       "0    A1       a     NaN  indefinite article   \n",
       "1    A1      an     NaN  indefinite article   \n",
       "2    A1   about     NaN         prep., adv.   \n",
       "3    A1   above     NaN         prep., adv.   \n",
       "4    A1  across     NaN         prep., adv.   \n",
       "\n",
       "                                     file  \n",
       "0  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "1  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "2  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "3  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "4  American_Oxford_3000_by_CEFR_level.pdf  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# считываем словарь и создаём дополнительный объект\n",
    "df_words = pd.read_excel(DICTIONARY)\n",
    "df_words.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d6162e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>word</th>\n",
       "      <th>comment</th>\n",
       "      <th>semantic</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>an</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1</td>\n",
       "      <td>about</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>above</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1</td>\n",
       "      <td>across</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  level    word comment            semantic  \\\n",
       "0    A1       a     NaN  indefinite article   \n",
       "1    A1      an     NaN  indefinite article   \n",
       "2    A1   about     NaN         prep., adv.   \n",
       "3    A1   above     NaN         prep., adv.   \n",
       "4    A1  across     NaN         prep., adv.   \n",
       "\n",
       "                                     file  \n",
       "0  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "1  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "2  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "3  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "4  American_Oxford_3000_by_CEFR_level.pdf  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "503cdd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oxford', 'USA']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_replace = {'American_Oxford_3000_by_CEFR_level.pdf':'USA', 'American_Oxford_5000_by_CEFR_level.pdf':'USA', 'The_Oxford_3000_by_CEFR_level.pdf':'Oxford',\n",
    "'The_Oxford_5000_by_CEFR_level.pdf':'Oxford'}\n",
    "df_words['file'] = df_words['file'].replace(file_replace, regex=True)\n",
    "sorted(df_words['file'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "81143579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>word</th>\n",
       "      <th>comment</th>\n",
       "      <th>semantic</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>an</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1</td>\n",
       "      <td>about</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>above</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1</td>\n",
       "      <td>across</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  level    word comment            semantic file\n",
       "0    A1       a     NaN  indefinite article  USA\n",
       "1    A1      an     NaN  indefinite article  USA\n",
       "2    A1   about     NaN         prep., adv.  USA\n",
       "3    A1   above     NaN         prep., adv.  USA\n",
       "4    A1  across     NaN         prep., adv.  USA"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5eca416e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1</th>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2</th>\n",
       "      <td>1471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word\n",
       "level      \n",
       "A1      761\n",
       "A2      767\n",
       "B1      775\n",
       "B2     1471\n",
       "C1     1455"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# отсортируем слова и посмотрим сколько слов какого уровня у нас есть:\n",
    "df_words = df_words.sort_values(by=['word', 'level'], ascending=True)\n",
    "df_words = df_words.drop_duplicates(subset=['word'], keep='last')\n",
    "df_words.head()\n",
    "df_words.groupby('level').agg({'word':'count'}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "e98c75bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраним словарь:\n",
    "with open(\"./df_words\",\"wb\") as fid_0: \n",
    "    dump(df_words,fid_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4edcedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Нормализация субтитров основного рабочего файла:\n",
    "# Проведем токенизацию субтитров\n",
    "# далее уберем все стоп-слова\n",
    "# Проведем лемантизацию и далее найдем частоту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "45245286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_prep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[little, girl, i, a, policeman, little, girl, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[mom, right, here, any, luck, how, do, we, tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[that, right, you, heard, me, bitch, you, got,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[what, nothing, it, not, nothing, it, always, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, i, do, know,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                           subtitels level  \\\n",
       "0          little girl i'm a policeman little gir...    A2   \n",
       "1   mom right here  any luck how do we tell if th...    A2   \n",
       "2   that's right you heard me bitch you got a pro...    A2   \n",
       "3   what nothing it's not nothing it's always som...    A2   \n",
       "4  walkie talkie squawks morgan i don't know if y...    A2   \n",
       "\n",
       "                                           subs_prep  \n",
       "0  [little, girl, i, a, policeman, little, girl, ...  \n",
       "1  [mom, right, here, any, luck, how, do, we, tel...  \n",
       "2  [that, right, you, heard, me, bitch, you, got,...  \n",
       "3  [what, nothing, it, not, nothing, it, always, ...  \n",
       "4  [walkie, talkie, squawks, morgan, i, do, know,...  "
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def tokenize(column):    \n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()] \n",
    "\n",
    "prep_text = [tokenize(text) for text in final_df['subtitels'].astype('str')]\n",
    "\n",
    "final_df['subs_prep'] = prep_text\n",
    "final_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ca0a1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stopwords(text):\n",
    "    tokens_without_sw = [word for word in text if not word in english_stopwords] \n",
    "    return tokens_without_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a6acdffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_prep</th>\n",
       "      <th>subs_nomal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[little, girl, i, a, policeman, little, girl, ...</td>\n",
       "      <td>[little, girl, policeman, little, girl, afraid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[mom, right, here, any, luck, how, do, we, tel...</td>\n",
       "      <td>[mom, right, luck, tell, poison, uh, one, sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[that, right, you, heard, me, bitch, you, got,...</td>\n",
       "      <td>[right, heard, bitch, got, problem, bring, man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[what, nothing, it, not, nothing, it, always, ...</td>\n",
       "      <td>[nothing, nothing, always, something, dad, tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, i, do, know,...</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, know, know, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                           subtitels level  \\\n",
       "0          little girl i'm a policeman little gir...    A2   \n",
       "1   mom right here  any luck how do we tell if th...    A2   \n",
       "2   that's right you heard me bitch you got a pro...    A2   \n",
       "3   what nothing it's not nothing it's always som...    A2   \n",
       "4  walkie talkie squawks morgan i don't know if y...    A2   \n",
       "\n",
       "                                           subs_prep  \\\n",
       "0  [little, girl, i, a, policeman, little, girl, ...   \n",
       "1  [mom, right, here, any, luck, how, do, we, tel...   \n",
       "2  [that, right, you, heard, me, bitch, you, got,...   \n",
       "3  [what, nothing, it, not, nothing, it, always, ...   \n",
       "4  [walkie, talkie, squawks, morgan, i, do, know,...   \n",
       "\n",
       "                                          subs_nomal  \n",
       "0  [little, girl, policeman, little, girl, afraid...  \n",
       "1  [mom, right, luck, tell, poison, uh, one, sure...  \n",
       "2  [right, heard, bitch, got, problem, bring, man...  \n",
       "3  [nothing, nothing, always, something, dad, tea...  \n",
       "4  [walkie, talkie, squawks, morgan, know, know, ...  "
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выполняем нормализацию данных\n",
    "final_df['subs_nomal'] = final_df['subs_prep'].apply(token_stopwords)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "fbed5557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_prep</th>\n",
       "      <th>subs_nomal</th>\n",
       "      <th>B2</th>\n",
       "      <th>A1</th>\n",
       "      <th>B1</th>\n",
       "      <th>A2</th>\n",
       "      <th>C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[little, girl, i, a, policeman, little, girl, ...</td>\n",
       "      <td>[little, girl, policeman, little, girl, afraid...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[mom, right, here, any, luck, how, do, we, tel...</td>\n",
       "      <td>[mom, right, luck, tell, poison, uh, one, sure...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[that, right, you, heard, me, bitch, you, got,...</td>\n",
       "      <td>[right, heard, bitch, got, problem, bring, man...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[what, nothing, it, not, nothing, it, always, ...</td>\n",
       "      <td>[nothing, nothing, always, something, dad, tea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, i, do, know,...</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, know, know, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                           subtitels level  \\\n",
       "0          little girl i'm a policeman little gir...    A2   \n",
       "1   mom right here  any luck how do we tell if th...    A2   \n",
       "2   that's right you heard me bitch you got a pro...    A2   \n",
       "3   what nothing it's not nothing it's always som...    A2   \n",
       "4  walkie talkie squawks morgan i don't know if y...    A2   \n",
       "\n",
       "                                           subs_prep  \\\n",
       "0  [little, girl, i, a, policeman, little, girl, ...   \n",
       "1  [mom, right, here, any, luck, how, do, we, tel...   \n",
       "2  [that, right, you, heard, me, bitch, you, got,...   \n",
       "3  [what, nothing, it, not, nothing, it, always, ...   \n",
       "4  [walkie, talkie, squawks, morgan, i, do, know,...   \n",
       "\n",
       "                                          subs_nomal  B2  A1  B1  A2  C1  \n",
       "0  [little, girl, policeman, little, girl, afraid...   0   0   0   0   0  \n",
       "1  [mom, right, luck, tell, poison, uh, one, sure...   0   0   0   0   0  \n",
       "2  [right, heard, bitch, got, problem, bring, man...   0   0   0   0   0  \n",
       "3  [nothing, nothing, always, something, dad, tea...   0   0   0   0   0  \n",
       "4  [walkie, talkie, squawks, morgan, know, know, ...   0   0   0   0   0  "
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_words = {}\n",
    "# создаём новые колонки по уровням в основном датасете:\n",
    "for level in df_words['level'].unique():\n",
    "    final_df[level] = 0\n",
    "    \n",
    "    dict_words[level] = df_words.loc[df_words['level'] == level, 'word'].values\n",
    "    \n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "07211e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем доли слов определённых категорий в фильме, используя словарь с указанием уровня:\n",
    "def level_words(row):     \n",
    "    words = row['subs_lemm']\n",
    "        \n",
    "    for level in df_words['level'].unique():\n",
    "        row[level] = len([word for word in words if word.lower() in dict_words[level]]) / len(words)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "1cdc96cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_prep</th>\n",
       "      <th>subs_nomal</th>\n",
       "      <th>B2</th>\n",
       "      <th>A1</th>\n",
       "      <th>B1</th>\n",
       "      <th>A2</th>\n",
       "      <th>C1</th>\n",
       "      <th>subs_lemm</th>\n",
       "      <th>sub_for_ml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[little, girl, i, a, policeman, little, girl, ...</td>\n",
       "      <td>[little, girl, policeman, little, girl, afraid...</td>\n",
       "      <td>0.077070</td>\n",
       "      <td>0.382166</td>\n",
       "      <td>0.065605</td>\n",
       "      <td>0.141401</td>\n",
       "      <td>0.029299</td>\n",
       "      <td>[little, girl, policeman, little, girl, afraid...</td>\n",
       "      <td>little girl policeman little girl afraid okay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[mom, right, here, any, luck, how, do, we, tel...</td>\n",
       "      <td>[mom, right, luck, tell, poison, uh, one, sure...</td>\n",
       "      <td>0.087417</td>\n",
       "      <td>0.364901</td>\n",
       "      <td>0.080795</td>\n",
       "      <td>0.123841</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>[mom, right, luck, tell, poison, uh, one, sure...</td>\n",
       "      <td>mom right luck tell poison uh one sure way kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[that, right, you, heard, me, bitch, you, got,...</td>\n",
       "      <td>[right, heard, bitch, got, problem, bring, man...</td>\n",
       "      <td>0.080671</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.053955</td>\n",
       "      <td>0.122053</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>[right, heard, bitch, got, problem, bring, man...</td>\n",
       "      <td>right heard bitch got problem bring man enough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[what, nothing, it, not, nothing, it, always, ...</td>\n",
       "      <td>[nothing, nothing, always, something, dad, tea...</td>\n",
       "      <td>0.079605</td>\n",
       "      <td>0.350959</td>\n",
       "      <td>0.073213</td>\n",
       "      <td>0.120860</td>\n",
       "      <td>0.017432</td>\n",
       "      <td>[nothing, nothing, always, something, dad, tea...</td>\n",
       "      <td>nothing nothing always something dad teach tie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, i, do, know,...</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, know, know, ...</td>\n",
       "      <td>0.080634</td>\n",
       "      <td>0.369330</td>\n",
       "      <td>0.092153</td>\n",
       "      <td>0.125990</td>\n",
       "      <td>0.026638</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, know, know, ...</td>\n",
       "      <td>walkie talkie squawks morgan know know hear ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "\n",
       "                                           subtitels level  \\\n",
       "0          little girl i'm a policeman little gir...    A2   \n",
       "1   mom right here  any luck how do we tell if th...    A2   \n",
       "2   that's right you heard me bitch you got a pro...    A2   \n",
       "3   what nothing it's not nothing it's always som...    A2   \n",
       "4  walkie talkie squawks morgan i don't know if y...    A2   \n",
       "\n",
       "                                           subs_prep  \\\n",
       "0  [little, girl, i, a, policeman, little, girl, ...   \n",
       "1  [mom, right, here, any, luck, how, do, we, tel...   \n",
       "2  [that, right, you, heard, me, bitch, you, got,...   \n",
       "3  [what, nothing, it, not, nothing, it, always, ...   \n",
       "4  [walkie, talkie, squawks, morgan, i, do, know,...   \n",
       "\n",
       "                                          subs_nomal        B2        A1  \\\n",
       "0  [little, girl, policeman, little, girl, afraid...  0.077070  0.382166   \n",
       "1  [mom, right, luck, tell, poison, uh, one, sure...  0.087417  0.364901   \n",
       "2  [right, heard, bitch, got, problem, bring, man...  0.080671  0.356207   \n",
       "3  [nothing, nothing, always, something, dad, tea...  0.079605  0.350959   \n",
       "4  [walkie, talkie, squawks, morgan, know, know, ...  0.080634  0.369330   \n",
       "\n",
       "         B1        A2        C1  \\\n",
       "0  0.065605  0.141401  0.029299   \n",
       "1  0.080795  0.123841  0.022517   \n",
       "2  0.053955  0.122053  0.029859   \n",
       "3  0.073213  0.120860  0.017432   \n",
       "4  0.092153  0.125990  0.026638   \n",
       "\n",
       "                                           subs_lemm  \\\n",
       "0  [little, girl, policeman, little, girl, afraid...   \n",
       "1  [mom, right, luck, tell, poison, uh, one, sure...   \n",
       "2  [right, heard, bitch, got, problem, bring, man...   \n",
       "3  [nothing, nothing, always, something, dad, tea...   \n",
       "4  [walkie, talkie, squawks, morgan, know, know, ...   \n",
       "\n",
       "                                          sub_for_ml  \n",
       "0  little girl policeman little girl afraid okay ...  \n",
       "1  mom right luck tell poison uh one sure way kno...  \n",
       "2  right heard bitch got problem bring man enough...  \n",
       "3  nothing nothing always something dad teach tie...  \n",
       "4  walkie talkie squawks morgan know know hear ma...  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лемантизация:\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemm_texts_list_1 =[]\n",
    "lemm_texts_list_2 = []\n",
    "for text in final_df['subs_nomal']:\n",
    "    text_lem = [morph.parse(word)[0].normal_form for word in text]\n",
    "    if len(text_lem) <= 1:\n",
    "        lemm_texts_list_1.append('')\n",
    "        lemm_texts_list_2.append('')\n",
    "        continue\n",
    "    lemm_texts_list_1.append(text_lem)\n",
    "    lemm_texts_list_2.append(' '.join(text_lem))\n",
    "final_df['subs_lemm']= lemm_texts_list_1\n",
    "final_df = final_df[final_df['subs_lemm']!='']\n",
    "final_df = final_df.apply(level_words, axis=1)\n",
    "final_df['sub_for_ml']= lemm_texts_list_2\n",
    "final_df = final_df[final_df['sub_for_ml']!='']\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e674bf",
   "metadata": {},
   "source": [
    "**ПОДГОТОВКА ДАННЫХ ДЛЯ ОБУЧЕНИЯ МОДЕЛЕЙ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "46a1ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: 122\n",
      "Валидационная выборка: 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71     B2\n",
       "104    B2\n",
       "68     B2\n",
       "154    C1\n",
       "120    B2\n",
       "Name: level, dtype: object"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_df['sub_for_ml'] = final_df['sub_for_ml'].astype('str')\n",
    "features = final_df[['sub_for_ml', 'A1', 'A2', 'B1', 'B2', 'C1']]\n",
    "target = final_df['level']\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size=0.25, random_state=12345, shuffle=True)\n",
    "\n",
    "# размеры выборок\n",
    "print(f'Обучающая выборка:', features_train.shape[0])\n",
    "print(f'Валидационная выборка:', features_valid.shape[0])\n",
    "target_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "cb8aafdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_for_ml</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>amazing woman ever met think rachel elizabeth ...</td>\n",
       "      <td>0.738726</td>\n",
       "      <td>-0.567726</td>\n",
       "      <td>-1.118052</td>\n",
       "      <td>-0.744066</td>\n",
       "      <td>0.318372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>suing entire firm fraud never saw thing life d...</td>\n",
       "      <td>-0.043931</td>\n",
       "      <td>-0.667788</td>\n",
       "      <td>-0.431354</td>\n",
       "      <td>0.686724</td>\n",
       "      <td>1.049768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>paramount news brings special coverage princes...</td>\n",
       "      <td>1.296070</td>\n",
       "      <td>0.681081</td>\n",
       "      <td>1.345819</td>\n",
       "      <td>-1.049212</td>\n",
       "      <td>0.500117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>previously suits guilty bribing foreign govern...</td>\n",
       "      <td>-0.019008</td>\n",
       "      <td>-0.545521</td>\n",
       "      <td>0.391896</td>\n",
       "      <td>-0.044058</td>\n",
       "      <td>0.588139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>come put table okay go yeah take wrap hey hell...</td>\n",
       "      <td>1.967389</td>\n",
       "      <td>-0.313581</td>\n",
       "      <td>-1.045802</td>\n",
       "      <td>-1.910925</td>\n",
       "      <td>0.314997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sub_for_ml        A1        A2  \\\n",
       "71   amazing woman ever met think rachel elizabeth ...  0.738726 -0.567726   \n",
       "104  suing entire firm fraud never saw thing life d... -0.043931 -0.667788   \n",
       "68   paramount news brings special coverage princes...  1.296070  0.681081   \n",
       "154  previously suits guilty bribing foreign govern... -0.019008 -0.545521   \n",
       "120  come put table okay go yeah take wrap hey hell...  1.967389 -0.313581   \n",
       "\n",
       "           B1        B2        C1  \n",
       "71  -1.118052 -0.744066  0.318372  \n",
       "104 -0.431354  0.686724  1.049768  \n",
       "68   1.345819 -1.049212  0.500117  \n",
       "154  0.391896 -0.044058  0.588139  \n",
       "120 -1.045802 -1.910925  0.314997  "
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_vec = features_train.copy()\n",
    "features_valid_vec = features_valid.copy()\n",
    "# Масштабируем количественные признаки и применим векторизацию к тексту:\n",
    "numeric = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train_vec[numeric])\n",
    "\n",
    "features_train_vec[numeric] = scaler.transform(features_train_vec[numeric])\n",
    "features_valid_vec[numeric] = scaler.transform(features_valid_vec[numeric])\n",
    "\n",
    "\n",
    "# посмотрим, что получилось\n",
    "features_train_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "aecca709",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "column_transformer = ColumnTransformer([('vect1', tfidf_vectorizer, 'sub_for_ml')], remainder='passthrough')\n",
    "\n",
    "features_train_vec = column_transformer.fit_transform(features_train_vec)\n",
    "features_valid_vec = column_transformer.transform(features_valid_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efe22d",
   "metadata": {},
   "source": [
    "**ОБУЧЕНИЕ МОДЕЛЕЙ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "bf4cadc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8426817881624195\n"
     ]
    }
   ],
   "source": [
    "#Логистическая регресси\n",
    "model_LR = LogisticRegression(n_jobs=3,C=1e5, solver='saga', \n",
    "                                           multi_class='multinomial',\n",
    "                                           max_iter=1000,\n",
    "                                           random_state=42)\n",
    "model_LR.fit(features_train_vec, target_train)\n",
    "pred = model_LR .predict(features_valid_vec)\n",
    "print(f\"F1 Score: {f1_score(target_valid, pred, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "2bdb650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# модель CatBoostClassifier\n",
    "def fit_model(train_pool, test_pool, **kwargs):\n",
    "    model = CatBoostClassifier(task_type='CPU', iterations = 5000,\n",
    "                               eval_metric='TotalF1', od_type='Iter', \n",
    "                               od_wait=500, **kwargs)\n",
    "    \n",
    "    return model.fit(train_pool, eval_set=test_pool, \n",
    "                     verbose=100, plot=True, \n",
    "                     use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6c8aefc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(data=features_train, label=target_train, \n",
    "                  text_features=['sub_for_ml'])\n",
    "valid_pool = Pool(data=features_valid, label=target_valid, \n",
    "                  text_features=['sub_for_ml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "9d54f8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5818e39b1be64f42b89e9a15ae5d8e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7705820\ttest: 0.7264808\tbest: 0.7264808 (0)\ttotal: 1.57s\tremaining: 2h 11m 1s\n",
      "100:\tlearn: 1.0000000\ttest: 0.8387050\tbest: 0.8387050 (50)\ttotal: 1m 27s\tremaining: 1h 11m 3s\n",
      "200:\tlearn: 1.0000000\ttest: 0.8387050\tbest: 0.8387050 (50)\ttotal: 2m 53s\tremaining: 1h 8m 59s\n",
      "300:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 4m 20s\tremaining: 1h 7m 49s\n",
      "400:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 5m 46s\tremaining: 1h 6m 15s\n",
      "500:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 7m 12s\tremaining: 1h 4m 42s\n",
      "600:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 8m 38s\tremaining: 1h 3m 13s\n",
      "700:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 10m 3s\tremaining: 1h 1m 42s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 0.8648637947\n",
      "bestIteration = 204\n",
      "\n",
      "Shrink model to first 205 iterations.\n"
     ]
    }
   ],
   "source": [
    "model = fit_model(train_pool, valid_pool, learning_rate=0.35,\n",
    "                  dictionaries = [{\n",
    "                      'dictionary_id':'Word',\n",
    "                      'max_dictionary_size': '50000'\n",
    "                  }],\n",
    "                 feature_calcers = ['BoW:top_tokens_count=10000'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342482f3",
   "metadata": {},
   "source": [
    "Вторая модель дала лучшую метрику, будем в дальнейшем использовать ее для работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "5fde6498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['C1']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['C1']\n",
      " ['C1']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['C1']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B1']\n",
      " ['B2']\n",
      " ['A2']\n",
      " ['B1']\n",
      " ['C1']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['C1']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['C1']\n",
      " ['A2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']\n",
      " ['B2']]\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.predict(features_valid)\n",
    "print(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "842e06ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature  importance\n",
      "0  sub_for_ml   99.930776\n",
      "1          A1    0.029118\n",
      "2          C1    0.022457\n",
      "3          B2    0.011182\n",
      "4          A2    0.006466\n",
      "5          B1    0.000000\n"
     ]
    }
   ],
   "source": [
    "#важные признаки, которые выделила модель\n",
    "importances = model.feature_importances_\n",
    "len(importances)\n",
    "feature_list = list(features_valid.columns)\n",
    "feature_results = pd.DataFrame({'feature': feature_list,'importance': importances})\n",
    "feature_results = feature_results.sort_values('importance',ascending = False).reset_index(drop=True)\n",
    "\n",
    "print(feature_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8466cf98",
   "metadata": {},
   "source": [
    "Итак, большей частью модель использовала субтитры и совсем немного долевое распределение слов по уровням"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "7d0c1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраним данную модель с наилучшими параметрами для дальнейшего использования:\n",
    "import pickle\n",
    "from pickle import dump,load\n",
    "with open(\"./model_finish\",\"wb\") as fid: \n",
    "    dump(model,fid)\n",
    "with open(\"./scaler\",\"wb\") as fid_1: \n",
    "    dump(scaler,fid_1)\n",
    "with open(\"./column_transformer\",\"wb\") as fid_1: \n",
    "    dump(column_transformer,fid_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5d192",
   "metadata": {},
   "source": [
    "**ВЫВОДЫ:**\n",
    "    На основании полученных метрик F1 для дальнейшей работы будем использовать модель CatBoostClassifier. Именно ее сохраним и представим заказчику через платформу Streamlit. Данная модель предсказывает уровень сложности фильмов на основе анализа субтитров и долей присутсвия слов различных категорий слов из словаря в субтитрах фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5391b4b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
