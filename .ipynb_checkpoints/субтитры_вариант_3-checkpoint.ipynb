{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "589346c7",
   "metadata": {},
   "source": [
    "**ОПРЕДЕЛЕНИЕ УРОВНЯ СЛОЖНОСТИ АНГЛОЯЗЫЧНЫХ ФИЛЬМОВ**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f195e231",
   "metadata": {},
   "source": [
    "Задача: разработать модель машинного обучения, которая позволит по анлаизу субтитров оценить уровень сложности англоязычного фильма"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f43c7",
   "metadata": {},
   "source": [
    "Заказчиком предоставлены следующие материалы:\n",
    "  - Файлы с субтитрами англоязычных фильмов, рассортированные по уровням сложности, согласно шкале CEFR: A1, A2, B1, B2, C1, C2. Каждый файл имеет формат .srt\n",
    "  - Файл в формате excel, сдержащий наименование англоязычных фильмов с уже известным уровнем сложности.\n",
    "  - Словари: Американский на 3000 слов и на 5000 слов, Оксфордский на 3000 слов и на 5000 слов в формате pdf. В данных словарях для каждого уровня сложности из системы CEFR прописан набор слова."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2380eb",
   "metadata": {},
   "source": [
    "В процессе работы нам необходимо:\n",
    "\n",
    "- провести анализ предоставленного заказчиком материала\n",
    "- подготовить материал для машинного обучения\n",
    "- создать и обучить несколькоих моделей, предсказывающих уровень сложности субтитров\n",
    "- определить оптимальную модель на основании выбранной для анализа метрики\n",
    "- создать приложение на плтаформе streamlit для демонстрации работы модели\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00769595",
   "metadata": {},
   "source": [
    "**ИМПОРТИРУЕМ НЕОБХОДИМЫЕ ДЛЯ РАБОТЫ БИБЛИОТЕКИ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "70172513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysrt in c:\\users\\zst\\anaconda3\\lib\\site-packages (1.1.2)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: chardet in c:\\users\\zst\\anaconda3\\lib\\site-packages (from pysrt) (4.0.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pysrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f6bdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\zst\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from PyPDF2) (4.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edbf35f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\zst\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\zst\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\zst\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\zst\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\zst\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e8841bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\zst\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (1.7.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (1.21.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (3.5.1)\n",
      "Requirement already satisfied: graphviz in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (1.4.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (5.6.0)\n",
      "Requirement already satisfied: six in c:\\users\\zst\\anaconda3\\lib\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (4.25.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (9.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from plotly->catboost) (8.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8739b86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\zst\\anaconda3\\lib\\site-packages (3.3.5)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: scipy in c:\\users\\zst\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\zst\\anaconda3\\lib\\site-packages (from lightgbm) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from lightgbm) (1.2.2)\n",
      "Requirement already satisfied: wheel in c:\\users\\zst\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\zst\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eb95dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zst\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pysrt\n",
    "import re\n",
    "import chardet\n",
    "import string\n",
    "import pymorphy2\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import pickle\n",
    "from pickle import dump,load\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None # для игнорирования предупреждения\n",
    "# уберем ограничения на количество выводимых столбцов, что бы просмотреть все столбцы\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from functools import reduce\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dbbbd4",
   "metadata": {},
   "source": [
    "**РАЗБЕРЕМ НАБОР ФАЙЛОВ Subtitles_all**\n",
    "______________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7424af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Введем константы для указания путей к файлам\n",
    "PATH_SUBTITLES_ALL = \"D:/jupiter_projects/мастерская_2/English_level/English_level/English_scores/Subtitles_all\" \n",
    "MOV_LAB_PATH = \"D:/jupiter_projects/мастерская_2/English_level/English_level/English_scores/movies_labels.xlsx\"\n",
    "DICTIONARY = \"D:/jupiter_projects/мастерская_2/English_level/English_level/Oxford_CEFR_level/dictionary.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ea9051a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция очистки субтитров от лишних символов:\n",
    "\n",
    "# Константы:\n",
    "HTML = r'<.*?>' # html тэги меняем на пробел\n",
    "TAG = r'{.*?}' # тэги меняем на пробел\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]' # комменты в скобках меняем на пробел\n",
    "UPPER = r'[[A-Za-z ]+[\\:\\]]' # указания на того кто говорит (BOBBY:)\n",
    "LETTERS = r'[^a-zA-Z\\'.,!? ]' # все что не буквы меняем на пробел \n",
    "SPACES = r'([ ])\\1+' # повторяющиеся пробелы меняем на один пробел\n",
    "DOTS = r'[\\.]+' # многоточие меняем на точку\n",
    "SYMB = r\"[^\\w\\d'\\s]\" # знаки препинания кроме апострофа\n",
    "\n",
    "def clean_subs(sentence):\n",
    "    sentence = re.sub(r'\\n',' ', sentence)\n",
    "    sentence = re.sub(HTML, ' ', sentence) # html тэги меняем на пробел\n",
    "    sentence = re.sub(TAG, ' ', sentence) # тэги меняем на пробел\n",
    "    sentence = re.sub(COMMENTS, ' ', sentence) # комменты в скобках меняем на пробел\n",
    "    sentence = re.sub(UPPER, ' ', sentence) # указания на того кто говорит (BOBBY:)\n",
    "    sentence = re.sub(LETTERS, ' ', sentence) # все что не буквы меняем на пробел\n",
    "    sentence = re.sub(DOTS, r'.', sentence) # многоточие меняем на точку\n",
    "    sentence = re.sub(SPACES, r'\\1', sentence) # повторяющиеся пробелы меняем на один пробел\n",
    "    sentence = re.sub(SYMB, '', sentence) # знаки препинания кроме апострофа на пустую строку\n",
    "    sentence = re.sub('www', '', sentence) # кое-где остаётся www, то же меняем на пустую строку\n",
    "    sentence = re.sub(r'(\\ufeff)?\\d+\\t?\\d{1,2}:\\d{1,2}:\\d{1,2},\\d{1,5}\\t?\\d{1,3}:\\d{1,2}:\\d{1,2},\\d{1,5}\\t?', '', sentence) # Удаление временной метки\n",
    "    sentence = sentence.lstrip() # обрезка пробелов в начале и в конце\n",
    "    sentence = sentence.encode('ascii', 'ignore').decode() # удаляем все что не ascii символы   \n",
    "    sentence = sentence.lower() # текст в нижний регистр     \n",
    "    return sentence\n",
    "# функция чтения субтитров в файле\n",
    "def read_sub(subs):\n",
    "    text = []\n",
    "    for i in subs:\n",
    "        sentence = clean_subs(i.text_without_tags)\n",
    "        text.append(sentence)\n",
    "    return ' '.join(text)\n",
    "# функция чтения файлов из общего каталога\n",
    "def read_file (dirname, filename):\n",
    "    fullpath = os.path.join (dirname, filename)\n",
    "  \n",
    "    try:\n",
    "        enc = chardet.detect(open(fullpath, \"rb\").read())['encoding']\n",
    "        subs = pysrt.open(fullpath, enc)\n",
    "    except Exception as e: \n",
    "        print(e)\n",
    "        print('файл не читается', fullpath, filename)\n",
    "        return False\n",
    "    return read_sub(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "819b2ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Walking Dead-S01E01-Days Gone Bye.English</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Walking Dead-S01E02-Guts.English</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead-S01E03-Tell It To The Frogs.E...</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Walking Dead-S01E04-Vatos.English</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Walking Dead-S01E05-Wildfire.English</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Walking Dead-S01E06-TS-19.English</td>\n",
       "      <td>hey hey whoa whoa whoa whoa ma'am ma'am pleas...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AmericanBeauty1999.BRRip</td>\n",
       "      <td>i need a father who's a role model not some ho...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Angelas.Christmas.Wish.2020</td>\n",
       "      <td>where are we going come on angela but who'll m...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indiana Jones And The Last Crusade DVDRip Xvid...</td>\n",
       "      <td>dismount herman's horse sick chaps no one wand...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mechanic-resurrection_</td>\n",
       "      <td>mr santos so good to see you we saved your usu...</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie  \\\n",
       "0      The Walking Dead-S01E01-Days Gone Bye.English   \n",
       "1               The Walking Dead-S01E02-Guts.English   \n",
       "2  The Walking Dead-S01E03-Tell It To The Frogs.E...   \n",
       "3              The Walking Dead-S01E04-Vatos.English   \n",
       "4           The Walking Dead-S01E05-Wildfire.English   \n",
       "5              The Walking Dead-S01E06-TS-19.English   \n",
       "6                           AmericanBeauty1999.BRRip   \n",
       "7                        Angelas.Christmas.Wish.2020   \n",
       "8  Indiana Jones And The Last Crusade DVDRip Xvid...   \n",
       "9                             mechanic-resurrection_   \n",
       "\n",
       "                                           subtitels level  \n",
       "0          little girl i'm a policeman little gir...    A2  \n",
       "1   mom right here  any luck how do we tell if th...    A2  \n",
       "2   that's right you heard me bitch you got a pro...    A2  \n",
       "3   what nothing it's not nothing it's always som...    A2  \n",
       "4  walkie talkie squawks morgan i don't know if y...    A2  \n",
       "5   hey hey whoa whoa whoa whoa ma'am ma'am pleas...    A2  \n",
       "6  i need a father who's a role model not some ho...    B1  \n",
       "7  where are we going come on angela but who'll m...    B1  \n",
       "8  dismount herman's horse sick chaps no one wand...    B1  \n",
       "9  mr santos so good to see you we saved your usu...    B1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# считываем файлы субтитров:\n",
    "files = []\n",
    "title = []\n",
    "level = []\n",
    "for dirpath, _, filenames in os.walk(PATH_SUBTITLES_ALL):   \n",
    "    for file in filenames:\n",
    "        title.append(file.replace('.srt', ''))\n",
    "        level.append(os.path.basename(dirpath))\n",
    "        result = read_file(dirpath, file) \n",
    "        files.append(result)\n",
    "       \n",
    "        \n",
    "# создаем датафрем:\n",
    "data = pd.DataFrame({'movie':title, 'subtitels':files, 'level':level})  \n",
    "\n",
    "#выведем первые 10 строк датафрейма и информацию о датасете:\n",
    "data.head(10)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b018495",
   "metadata": {},
   "source": [
    "***РАЗБЕРЕМ ДОКУМЕНТ Movies_Lables***\n",
    "_________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebd4540",
   "metadata": {},
   "source": [
    "Скачаем и переведем в датасет экселевский файл с наименованием фильмов и указанием уровня сложности данного фильма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f1ea088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>All_dogs_go_to_heaven(1989)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>An_American_tail(1986)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Babe(1995)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Back_to_the_future(1985)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Banking_On_Bitcoin(2016)</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Batman_begins(2005)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Beauty_and_the_beast(2017)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Before_I_go_to_sleep(2014)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Before_sunrise(1995)</td>\n",
       "      <td>B1, B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Before_sunset(2004)</td>\n",
       "      <td>B1, B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                             movie   level\n",
       "0    0         10_Cloverfield_lane(2016)      B1\n",
       "1    1  10_things_I_hate_about_you(1999)      B1\n",
       "2    2              A_knights_tale(2001)      B2\n",
       "3    3              A_star_is_born(2018)      B2\n",
       "4    4                     Aladdin(1992)  A2/A2+\n",
       "5    5       All_dogs_go_to_heaven(1989)  A2/A2+\n",
       "6    6            An_American_tail(1986)  A2/A2+\n",
       "7    7                        Babe(1995)  A2/A2+\n",
       "8    8          Back_to_the_future(1985)  A2/A2+\n",
       "9    9          Banking_On_Bitcoin(2016)      C1\n",
       "10  10               Batman_begins(2005)  A2/A2+\n",
       "11  11        Beauty_and_the_beast(2017)      B2\n",
       "12  12        Before_I_go_to_sleep(2014)      B2\n",
       "13  13              Before_sunrise(1995)  B1, B2\n",
       "14  14               Before_sunset(2004)  B1, B2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_labels = pd.read_excel(MOV_LAB_PATH)\n",
    "# Откорректируем названия столбцов для удобства дальнейшей работы с ними:\n",
    "movies_labels.columns = movies_labels.columns.str.lower()\n",
    "\n",
    "movies_labels.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce8c7d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов в наборе данных: 0\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на наличие полных дубликатов:\n",
    "print('Количество дубликатов в наборе данных:', movies_labels.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7624f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалим колонку id:\n",
    "movies_labels.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "405d131e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов в наборе данных: 2\n"
     ]
    }
   ],
   "source": [
    "# посмотрим на наличие дубликатов без колонки id:\n",
    "print('Количество дубликатов в наборе данных:', movies_labels.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d611245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f52ce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов в наборе данных после удаления: 0\n"
     ]
    }
   ],
   "source": [
    "# Удалим дубликаты:\n",
    "movies_labels.drop_duplicates(inplace=True)\n",
    "print('Количество дубликатов в наборе данных после удаления:', movies_labels.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e0fc1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2            101\n",
       "B1             53\n",
       "C1             40\n",
       "A2/A2+         26\n",
       "B1, B2          8\n",
       "A2              6\n",
       "A2/A2+, B1      5\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим на уникальные значения столбца Level:\n",
    "movies_labels['level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe72d0",
   "metadata": {},
   "source": [
    "Как мы видем, некоторые фильмы имеют двойное или даже тройное обочначение уровня фильма. Заменим такие значения на максимальные по уровню сложности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a69ddc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_labels['level'] = movies_labels['level'].replace('A2/A2+','A2').replace ('B1, B2','B1').replace ('A2/A2+, B1','B1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d626113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 239 entries, 0 to 240\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   movie   239 non-null    object\n",
      " 1   level   239 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 5.6+ KB\n"
     ]
    }
   ],
   "source": [
    "movies_labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0779b856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>All_dogs_go_to_heaven(1989)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>An_American_tail(1986)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Babe(1995)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Back_to_the_future(1985)</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Banking_On_Bitcoin(2016)</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              movie level\n",
       "0         10_Cloverfield_lane(2016)    B1\n",
       "1  10_things_I_hate_about_you(1999)    B1\n",
       "2              A_knights_tale(2001)    B2\n",
       "3              A_star_is_born(2018)    B2\n",
       "4                     Aladdin(1992)    A2\n",
       "5       All_dogs_go_to_heaven(1989)    A2\n",
       "6            An_American_tail(1986)    A2\n",
       "7                        Babe(1995)    A2\n",
       "8          Back_to_the_future(1985)    A2\n",
       "9          Banking_On_Bitcoin(2016)    C1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50f22afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10_Cloverfield_lane(2016)',\n",
       " '10_things_I_hate_about_you(1999)',\n",
       " 'A_knights_tale(2001)',\n",
       " 'A_star_is_born(2018)',\n",
       " 'Aladdin(1992)',\n",
       " 'All_dogs_go_to_heaven(1989)',\n",
       " 'AmericanBeauty1999.BRRip',\n",
       " 'An_American_tail(1986)',\n",
       " \"Angela's.Christmas.2018.WEBRip.Netflix\",\n",
       " 'Angelas.Christmas.Wish.2020',\n",
       " 'Babe(1995)',\n",
       " 'Back_to_the_future(1985)',\n",
       " 'Banking_On_Bitcoin(2016)',\n",
       " 'Batman_begins(2005)',\n",
       " 'Beauty_and_the_beast(2017)',\n",
       " 'Before_I_go_to_sleep(2014)',\n",
       " 'Before_sunrise(1995)',\n",
       " 'Before_sunset(2004)',\n",
       " 'Braveheart(1995)',\n",
       " 'Bridget_Jones_diary(2001)',\n",
       " 'Bridget_Joness_Baby',\n",
       " 'Bullet train',\n",
       " 'Cars(2006)',\n",
       " 'Cast_away(2000)',\n",
       " 'Catch_me_if_you_can(2002)',\n",
       " 'Charlie_and_the_Chocolate_Factory',\n",
       " 'Cinderella(1950)',\n",
       " 'Clueless(1995)',\n",
       " 'Collateral.Beauty.2016.720p.BRRip.x264.AAC-ETRG',\n",
       " 'Crazy4TV.com - Suits.S06E01.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E02.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E03.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E04.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E05.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E07.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E08.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E09.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E10.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E11.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E12.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E13.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E14.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E15.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Crazy4TV.com - Suits.S06E16.720p.BluRay.x265.HEVC.Crazy4ad',\n",
       " 'Deadpool(2016)',\n",
       " 'Despicable_Me(2010)',\n",
       " 'Die_hard(1988)',\n",
       " 'Downton Abbey - S01E01 - Episode 1.eng.SDH',\n",
       " 'Downton Abbey - S01E02 - Episode 2.eng.SDH',\n",
       " 'Downton Abbey - S01E03 - Episode 3.eng.SDH',\n",
       " 'Downton Abbey - S01E04 - Episode 4.eng.SDH',\n",
       " 'Downton Abbey - S01E05 - Episode 5.eng.SDH',\n",
       " 'Downton Abbey - S01E06 - Episode 6.eng.SDH',\n",
       " 'Downton Abbey - S01E07 - Episode 7.eng.SDH',\n",
       " 'Dredd(2012)',\n",
       " 'Dune(2021)',\n",
       " 'Enola_Holmes(2020)',\n",
       " 'Entrapment',\n",
       " 'Eurovision_song_contest_(2020)',\n",
       " 'Ferdinand(2017)',\n",
       " 'Fight_club(1999)',\n",
       " 'Finding_Nemo(2003)',\n",
       " 'Forrest_Gump(1994)',\n",
       " 'Ghosts.of.Girlfriends.Past.2009.BluRay.720p.x264.YIFY',\n",
       " 'Glass Onion',\n",
       " 'Good_Will_Hunting(1997)',\n",
       " 'Groundhog_day(1993)',\n",
       " 'Her(2013)',\n",
       " 'Home_alone(1990)',\n",
       " 'Hook(1991)',\n",
       " 'House_of_Gucci(2021)',\n",
       " 'Indiana Jones And The Last Crusade DVDRip Xvid -IZON-',\n",
       " 'Inside_out(2015)',\n",
       " 'Its_a_wonderful_life(1946)',\n",
       " 'Klaus(2019)',\n",
       " 'Knives_out(2019)',\n",
       " 'Kubo_and_the_two_strings(2016)',\n",
       " 'Liar_liar(1997)',\n",
       " 'Lightyear',\n",
       " 'Lion(2016)',\n",
       " 'Logan(2017)',\n",
       " 'Love_actually(2003)',\n",
       " 'Made_of_Honor(2008)',\n",
       " 'Mamma_Mia(2008)',\n",
       " 'Mary_Poppins_returns(2018)',\n",
       " 'Matilda(1996)',\n",
       " 'Matilda(2022)',\n",
       " 'Meet_the_parents(2000)',\n",
       " 'Men.In.Black.1997.720p.Bluray.x264-SEPTiC',\n",
       " 'Milada(2017)',\n",
       " 'Mona_Lisa_Smile(2003)',\n",
       " 'Moulin_Rouge(2001)',\n",
       " 'Mrs_Doubtfire(1993)',\n",
       " 'My_big_fat_Greek_wedding(2002)',\n",
       " 'Notting_Hill(1999)',\n",
       " 'Oceans_Eleven(2001)',\n",
       " 'Oceans_Twelve(2004)',\n",
       " 'Pirates_of_the_Caribbean(2003)',\n",
       " 'Pleasantville(1998)',\n",
       " 'Powder(1995)',\n",
       " 'Pulp_fiction(1994)',\n",
       " 'Rat.Race.2001.1080p.WEB-DL.DD5.1.H264-FGT',\n",
       " 'Ratatouille(2007)',\n",
       " 'Ready_or_not(2019)',\n",
       " 'Roman Holiday 1953 1080p WEBRip HEVC AAC',\n",
       " 'SOMM.Into.the.Bottle.2015.1080p.BluRay.x265-RARBG.en.srt',\n",
       " \"Secrets Of Her Majesty's Secret Service eng\",\n",
       " 'Seven.Worlds.One.Planet.S01E01.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E02.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E03.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E04.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E05.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E06.2160p.BluRay.Remux.eng',\n",
       " 'Seven.Worlds.One.Planet.S01E07.2160p.BluRay.Remux.eng',\n",
       " 'Shrek(2001)',\n",
       " 'Sleepless_in_Seattle(1993)',\n",
       " 'Soul(2020)',\n",
       " 'Spirit.Stallion.of.the.Cimarron.EN',\n",
       " 'Suits S04E01 EngSub',\n",
       " 'Suits S04E02 EngSub',\n",
       " 'Suits S04E03 EngSub',\n",
       " 'Suits S04E04 EngSub',\n",
       " 'Suits S04E05 EngSub',\n",
       " 'Suits S04E06 EngSub',\n",
       " 'Suits S04E07 EngSub',\n",
       " 'Suits S04E08 EngSub',\n",
       " 'Suits S04E09 EngSub',\n",
       " 'Suits S04E10 EngSub',\n",
       " 'Suits S04E11 EngSub',\n",
       " 'Suits S04E12 EngSub',\n",
       " 'Suits S04E13 EngSub',\n",
       " 'Suits S04E14 EngSub',\n",
       " 'Suits S04E15 EngSub',\n",
       " 'Suits S04E16 EngSub',\n",
       " 'Suits.Episode 1- Denial',\n",
       " 'Suits.Episode 10- Faith',\n",
       " 'Suits.Episode 11- Blowback',\n",
       " 'Suits.Episode 12- Live to Fight',\n",
       " \"Suits.Episode 13- God's Green Earth\",\n",
       " 'Suits.Episode 14- Self Defense',\n",
       " 'Suits.Episode 15- Tick Tock',\n",
       " 'Suits.Episode 16- 25th Hour',\n",
       " 'Suits.Episode 2- Compensation',\n",
       " 'Suits.Episode 3- No Refills',\n",
       " 'Suits.Episode 4- No Puedo Hacerlo',\n",
       " 'Suits.Episode 5- Toe to Toe',\n",
       " 'Suits.Episode 6- Privilege',\n",
       " 'Suits.Episode 7- Hitting Home',\n",
       " 'Suits.Episode 8- Mea Culpa',\n",
       " 'Suits.Episode 9- Uninvited Guests',\n",
       " 'Suits.S01E01.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E02.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E03.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E04.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E05.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E06.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E07.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E08.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E09.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E10.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E11.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S01E12.1080p.BluRay.AAC5.1.x265-DTG.02.EN',\n",
       " 'Suits.S02E01.HDTV.x264-AVS',\n",
       " 'Suits.S02E02.HDTV.x264-ASAP',\n",
       " 'Suits.S02E03.HDTV.x264-ASAP',\n",
       " 'Suits.S02E04.HDTV.x264-ASAP',\n",
       " 'Suits.S02E05.iNTERNAL.HDTV.x264-2HD',\n",
       " 'Suits.S02E06.All.In.HDTV.x264-FQM',\n",
       " 'Suits.S02E07.Sucker.Punch.PROPER.HDTV.x264-FQM',\n",
       " 'Suits.S02E08.HDTV.x264-EVOLVE',\n",
       " 'Suits.S02E09.HDTV.x264-ASAP',\n",
       " 'Suits.S02E10.HDTV.x264-ASAP',\n",
       " 'Suits.S02E11.HDTV.x264-ASAP',\n",
       " 'Suits.S02E12.HDTV.x264-ASAP',\n",
       " 'Suits.S02E13.REPACK.HDTV.x264-2HD',\n",
       " 'Suits.S02E14.HDTV.x264-ASAP',\n",
       " 'Suits.S02E15.HDTV.x264-ASAP',\n",
       " 'Suits.S02E16.HDTV.x264-2HD',\n",
       " 'Suits.S03E01.480pHDTV.x264-mSD',\n",
       " 'Suits.S03E02.720pHDTV.x264-mSD',\n",
       " 'Suits.S03E03.480pHDTV.x264-mSD',\n",
       " 'Suits.S03E04.480pHDTV.x264-mSD',\n",
       " 'Suits.S03E05.480p.HDTV.x264-mSD',\n",
       " 'Suits.S03E06.720p.HDTV.x264-mSD',\n",
       " 'Suits.S03E07.HDTV.x264-mSD',\n",
       " 'Suits.S03E08.480p.HDTV.x264-mSD',\n",
       " 'Suits.S03E09.480p.HDTV.x264-mSD',\n",
       " 'Suits.S03E10.HDTV.x264-mSD',\n",
       " 'Terminator_2_Judgment_Day_1991_roNy',\n",
       " 'The Grinch',\n",
       " 'The Secret Life of Pets.en',\n",
       " 'The Walking Dead-S01E01-Days Gone Bye.English',\n",
       " 'The Walking Dead-S01E02-Guts.English',\n",
       " 'The Walking Dead-S01E03-Tell It To The Frogs.English',\n",
       " 'The Walking Dead-S01E04-Vatos.English',\n",
       " 'The Walking Dead-S01E05-Wildfire.English',\n",
       " 'The Walking Dead-S01E06-TS-19.English',\n",
       " 'The.Notebook.DVDRip.XviD-DiAMOND',\n",
       " 'The_Devil_Wears_Prad',\n",
       " 'The_Fundamentals_of_Caring(2016)',\n",
       " 'The_Intern(2015)',\n",
       " 'The_Legend_of_Tarzan(2016)',\n",
       " 'The_Shawshank_redemption(1994)',\n",
       " 'The_blind_side(2009)',\n",
       " 'The_break-up(2006)',\n",
       " 'The_cabin_in_the_woods(2012)',\n",
       " 'The_fault_in_our_stars(2014)',\n",
       " 'The_graduate(1967)',\n",
       " 'The_greatest_showman(2017)',\n",
       " 'The_hangover(2009)',\n",
       " 'The_holiday(2006)',\n",
       " 'The_invisible_man(2020)',\n",
       " 'The_jungle_book(2016)',\n",
       " 'The_kings_speech(2010)',\n",
       " 'The_lion_king(1994)',\n",
       " 'The_lord_of_the_rings(2001)',\n",
       " 'The_man_called_Flintstone(1966)',\n",
       " 'The_secret_life_of_Walter_Mitty(2013)',\n",
       " 'The_social_network(2010)',\n",
       " 'The_terminal(2004)',\n",
       " 'The_terminator(1984)',\n",
       " 'The_theory_of_everything(2014)',\n",
       " 'The_usual_suspects(1995)',\n",
       " 'Thor: love and thunder',\n",
       " 'Titanic(1997)',\n",
       " 'Toy_story(1995)',\n",
       " 'Twilight(2008)',\n",
       " 'Up (2009)',\n",
       " \"Valentine's.Day.2010.Subtitles.YIFY\",\n",
       " 'Venom(2018)',\n",
       " 'Warm_bodies(2013)',\n",
       " 'We_are_the_Millers(2013)',\n",
       " 'While_You_Were_Sleeping(1995)',\n",
       " 'Zootopia(2016)',\n",
       " 'mechanic-resurrection_',\n",
       " 'z srt23 uk-bun Gullivers.Travels.1939.720p.BluRay.x264-CiNEFiLE']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "sorted(movies_labels['movie'].unique())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fcd4eb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10_cloverfield_lane(2016)',\n",
       " '10_things_i_hate_about_you(1999)',\n",
       " 'a_knights_tale(2001)',\n",
       " 'a_star_is_born(2018)',\n",
       " 'aladdin(1992)',\n",
       " 'all_dogs_go_to_heaven(1989)',\n",
       " 'americanbeauty1999.brrip',\n",
       " 'an_american_tail(1986)',\n",
       " \"angela's.christmas.2018.webrip.netflix\",\n",
       " 'angelas.christmas.wish.2020',\n",
       " 'babe(1995)',\n",
       " 'back_to_the_future(1985)',\n",
       " 'banking_on_bitcoin(2016)',\n",
       " 'batman_begins(2005)',\n",
       " 'beauty_and_the_beast(2017)',\n",
       " 'before_i_go_to_sleep(2014)',\n",
       " 'before_sunrise(1995)',\n",
       " 'before_sunset(2004)',\n",
       " 'braveheart(1995)',\n",
       " 'bridget_jones_diary(2001)',\n",
       " 'bridget_joness_baby',\n",
       " 'bullet train',\n",
       " 'cars(2006)',\n",
       " 'cast_away(2000)',\n",
       " 'catch_me_if_you_can(2002)',\n",
       " 'charlie_and_the_chocolate_factory',\n",
       " 'cinderella(1950)',\n",
       " 'clueless(1995)',\n",
       " 'collateral.beauty.2016.720p.brrip.x264.aac-etrg',\n",
       " 'crazy4tv.com - suits.s06e01.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e02.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e03.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e04.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e05.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e06.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e07.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e08.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e09.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e10.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e11.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e12.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e13.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e14.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e15.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e16.720p.bluray.x265.hevc.crazy4ad',\n",
       " 'deadpool(2016)',\n",
       " 'despicable_me(2010)',\n",
       " 'die_hard(1988)',\n",
       " 'downton abbey - s01e01 - episode 1.eng.sdh',\n",
       " 'downton abbey - s01e02 - episode 2.eng.sdh',\n",
       " 'downton abbey - s01e03 - episode 3.eng.sdh',\n",
       " 'downton abbey - s01e04 - episode 4.eng.sdh',\n",
       " 'downton abbey - s01e05 - episode 5.eng.sdh',\n",
       " 'downton abbey - s01e06 - episode 6.eng.sdh',\n",
       " 'downton abbey - s01e07 - episode 7.eng.sdh',\n",
       " 'dredd(2012)',\n",
       " 'dune(2021)',\n",
       " 'enola_holmes(2020)',\n",
       " 'entrapment',\n",
       " 'eurovision_song_contest_(2020)',\n",
       " 'ferdinand(2017)',\n",
       " 'fight_club(1999)',\n",
       " 'finding_nemo(2003)',\n",
       " 'forrest_gump(1994)',\n",
       " 'ghosts.of.girlfriends.past.2009.bluray.720p.x264.yify',\n",
       " 'glass onion',\n",
       " 'good_will_hunting(1997)',\n",
       " 'groundhog_day(1993)',\n",
       " 'her(2013)',\n",
       " 'home_alone(1990)',\n",
       " 'hook(1991)',\n",
       " 'house_of_gucci(2021)',\n",
       " 'indiana jones and the last crusade dvdrip xvid -izon-',\n",
       " 'inside_out(2015)',\n",
       " 'its_a_wonderful_life(1946)',\n",
       " 'klaus(2019)',\n",
       " 'knives_out(2019)',\n",
       " 'kubo_and_the_two_strings(2016)',\n",
       " 'liar_liar(1997)',\n",
       " 'lightyear',\n",
       " 'lion(2016)',\n",
       " 'logan(2017)',\n",
       " 'love_actually(2003)',\n",
       " 'made_of_honor(2008)',\n",
       " 'mamma_mia(2008)',\n",
       " 'mary_poppins_returns(2018)',\n",
       " 'matilda(1996)',\n",
       " 'matilda(2022)',\n",
       " 'mechanic-resurrection_',\n",
       " 'meet_the_parents(2000)',\n",
       " 'men.in.black.1997.720p.bluray.x264-septic',\n",
       " 'milada(2017)',\n",
       " 'mona_lisa_smile(2003)',\n",
       " 'moulin_rouge(2001)',\n",
       " 'mrs_doubtfire(1993)',\n",
       " 'my_big_fat_greek_wedding(2002)',\n",
       " 'notting_hill(1999)',\n",
       " 'oceans_eleven(2001)',\n",
       " 'oceans_twelve(2004)',\n",
       " 'pirates_of_the_caribbean(2003)',\n",
       " 'pleasantville(1998)',\n",
       " 'powder(1995)',\n",
       " 'pulp_fiction(1994)',\n",
       " 'rat.race.2001.1080p.web-dl.dd5.1.h264-fgt',\n",
       " 'ratatouille(2007)',\n",
       " 'ready_or_not(2019)',\n",
       " 'roman holiday(1953)',\n",
       " \"secrets of her majesty's secret service eng\",\n",
       " 'seven.worlds.one.planet.s01e01.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e02.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e03.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e04.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e05.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e06.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e07.2160p.bluray.remux.eng',\n",
       " 'shrek(2001)',\n",
       " 'sleepless_in_seattle(1993)',\n",
       " 'somm.into.the.bottle.2015.1080p.bluray.x265-rarbg.en.srt',\n",
       " 'soul(2020)',\n",
       " 'spirit.stallion.of.the.cimarron.en',\n",
       " 'suits s04e01 engsub',\n",
       " 'suits s04e02 engsub',\n",
       " 'suits s04e03 engsub',\n",
       " 'suits s04e04 engsub',\n",
       " 'suits s04e05 engsub',\n",
       " 'suits s04e06 engsub',\n",
       " 'suits s04e07 engsub',\n",
       " 'suits s04e08 engsub',\n",
       " 'suits s04e09 engsub',\n",
       " 'suits s04e10 engsub',\n",
       " 'suits s04e11 engsub',\n",
       " 'suits s04e12 engsub',\n",
       " 'suits s04e13 engsub',\n",
       " 'suits s04e14 engsub',\n",
       " 'suits s04e15 engsub',\n",
       " 'suits s04e16 engsub',\n",
       " 'suits.episode 1- denial',\n",
       " 'suits.episode 10- faith',\n",
       " 'suits.episode 11- blowback',\n",
       " 'suits.episode 12- live to fight',\n",
       " \"suits.episode 13- god's green earth\",\n",
       " 'suits.episode 14- self defense',\n",
       " 'suits.episode 15- tick tock',\n",
       " 'suits.episode 16- 25th hour',\n",
       " 'suits.episode 2- compensation',\n",
       " 'suits.episode 3- no refills',\n",
       " 'suits.episode 4- no puedo hacerlo',\n",
       " 'suits.episode 5- toe to toe',\n",
       " 'suits.episode 6- privilege',\n",
       " 'suits.episode 7- hitting home',\n",
       " 'suits.episode 8- mea culpa',\n",
       " 'suits.episode 9- uninvited guests',\n",
       " 'suits.s01e01.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e02.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e03.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e04.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e05.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e06.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e07.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e08.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e09.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e10.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e11.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e12.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s02e01.hdtv.x264-avs',\n",
       " 'suits.s02e02.hdtv.x264-asap',\n",
       " 'suits.s02e03.hdtv.x264-asap',\n",
       " 'suits.s02e04.hdtv.x264-asap',\n",
       " 'suits.s02e05.internal.hdtv.x264-2hd',\n",
       " 'suits.s02e06.all.in.hdtv.x264-fqm',\n",
       " 'suits.s02e07.sucker.punch.proper.hdtv.x264-fqm',\n",
       " 'suits.s02e08.hdtv.x264-evolve',\n",
       " 'suits.s02e09.hdtv.x264-asap',\n",
       " 'suits.s02e10.hdtv.x264-asap',\n",
       " 'suits.s02e11.hdtv.x264-asap',\n",
       " 'suits.s02e12.hdtv.x264-asap',\n",
       " 'suits.s02e13.repack.hdtv.x264-2hd',\n",
       " 'suits.s02e14.hdtv.x264-asap',\n",
       " 'suits.s02e15.hdtv.x264-asap',\n",
       " 'suits.s02e16.hdtv.x264-2hd',\n",
       " 'suits.s03e01.480phdtv.x264-msd',\n",
       " 'suits.s03e02.720phdtv.x264-msd',\n",
       " 'suits.s03e03.480phdtv.x264-msd',\n",
       " 'suits.s03e04.480phdtv.x264-msd',\n",
       " 'suits.s03e05.480p.hdtv.x264-msd',\n",
       " 'suits.s03e06.720p.hdtv.x264-msd',\n",
       " 'suits.s03e07.hdtv.x264-msd',\n",
       " 'suits.s03e08.480p.hdtv.x264-msd',\n",
       " 'suits.s03e09.480p.hdtv.x264-msd',\n",
       " 'suits.s03e10.hdtv.x264-msd',\n",
       " 'terminator_2_judgment_day_1991_rony',\n",
       " 'the grinch',\n",
       " 'the secret life of pets.en',\n",
       " 'the walking dead-s01e01-days gone bye.english',\n",
       " 'the walking dead-s01e02-guts.english',\n",
       " 'the walking dead-s01e03-tell it to the frogs.english',\n",
       " 'the walking dead-s01e04-vatos.english',\n",
       " 'the walking dead-s01e05-wildfire.english',\n",
       " 'the walking dead-s01e06-ts-19.english',\n",
       " 'the.notebook.dvdrip.xvid-diamond',\n",
       " 'the_blind_side(2009)',\n",
       " 'the_break-up(2006)',\n",
       " 'the_cabin_in_the_woods(2012)',\n",
       " 'the_devil_wears_prad',\n",
       " 'the_fault_in_our_stars(2014)',\n",
       " 'the_fundamentals_of_caring(2016)',\n",
       " 'the_graduate(1967)',\n",
       " 'the_greatest_showman(2017)',\n",
       " 'the_hangover(2009)',\n",
       " 'the_holiday(2006)',\n",
       " 'the_intern(2015)',\n",
       " 'the_invisible_man(2020)',\n",
       " 'the_jungle_book(2016)',\n",
       " 'the_kings_speech(2010)',\n",
       " 'the_legend_of_tarzan(2016)',\n",
       " 'the_lion_king(1994)',\n",
       " 'the_lord_of_the_rings(2001)',\n",
       " 'the_man_called_flintstone(1966)',\n",
       " 'the_secret_life_of_walter_mitty(2013)',\n",
       " 'the_shawshank_redemption(1994)',\n",
       " 'the_social_network(2010)',\n",
       " 'the_terminal(2004)',\n",
       " 'the_terminator(1984)',\n",
       " 'the_theory_of_everything(2014)',\n",
       " 'the_usual_suspects(1995)',\n",
       " 'thor: love and thunder',\n",
       " 'titanic(1997)',\n",
       " 'toy_story(1995)',\n",
       " 'twilight(2008)',\n",
       " 'up (2009)',\n",
       " \"valentine's.day.2010.subtitles.yify\",\n",
       " 'venom(2018)',\n",
       " 'warm_bodies(2013)',\n",
       " 'we_are_the_millers(2013)',\n",
       " 'while_you_were_sleeping(1995)',\n",
       " 'z srt23 uk-bun gullivers.travels.1939.720p.bluray.x264-cinefile',\n",
       " 'zootopia(2016)']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#movies_labels['movie'] = movies_labels['movie'].str.strip()#уберем лишнии пробелы в начале и в конце\n",
    "# создадим список замены других возможных комбинаций пробелов и дефисов:\n",
    "space_replace = {' _ ':' ','_ ':' ',',' :' ',' _':' ', ' - ':' ', '-':' ',r'\\.':' ',r'\\ ':' ','_':' ','╨':' ','+':'','/':' ','╨т':''}\n",
    "space_replace_1= {'z srt23 uk bun Gullivers Travels 1939 720p BluRay x264 CiNEFiLE':'Gullivers Travels(1939)','Day 2010 Subtitles YIFY':'Day(2010)','English':'','DVDRip XviD DiAMOND':'','1991 roNy':'(1991)',\n",
    "                'EngSub':'','HDTV x264':'','720p':'','480p':'','mSD':'','ASAP':'','EVOLVE':'','FQM':'','2HD':'','eng SDH':'',\n",
    "                'BluRay x265 HEVC Crazy4ad':''}\n",
    "space_replace_3 = {'Beauty 2016BRRip x264 AAC ETRG':'Beauty(2016)', 'Christmas 2018 WEBRip Netflix':'Christmas(2018)', 'Wish 2020':'Wish(2020)','BluRay x265 HEVC Crazy4ad':'',\n",
    "                'AmericanBeauty1999 BRRip':'American Beauty(1999)','Beauty 2016  BRRip x264 AAC ETRG':'Beauty(2016)','contest (2020)':'contest(2020)','DVDRip Xvid  IZON ':'','1080p BluRay AAC5 1 x265 DTG 02 EN':'',\n",
    "                '2160p BluRay Remux eng':'', 'contest (2020)':'contest(2020)','Past 2009 BluRay  x264 YIFY':'Past(2009)','Black 1997  Bluray x264 SEPTiC':'Black(1997)','Rat Race 2001 1080p WEB DL DD5 1 H264 FGT':'Rat Race(2001)',\n",
    "                'Roman Holiday 1953 1080p WEBRip HEVC AAC':'Roman Holiday(1953)', 'Bottle 2015 1080p BluRay x265 RARBG  srt':'Bottle(2015)','Service g':'Service','AVS':'','iNTERNAL':'','All In':'','Sucker Punch PROPER':'',\n",
    "                'REPACK':'', 'Day (1991)':'Day(1991)'}\n",
    "movies_labels['movie'] = movies_labels['movie'].replace(space_replace, regex=False)\n",
    "#movies_labels['movie'] = movies_labels['movie'].str.strip()\n",
    "movies_labels['movie'] = movies_labels['movie'].replace(space_replace_1, regex=False)\n",
    "#movies_labels['movie'] = movies_labels['movie'].str.strip()\n",
    "movies_labels['movie'] = movies_labels['movie'].replace(space_replace_3, regex=False)\n",
    "movies_labels['movie'] = movies_labels['movie'].str.strip()\n",
    "movies_labels['movie'] = movies_labels['movie'].str.lower()\n",
    "sorted(movies_labels['movie'].unique())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c15f395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ds_store',\n",
       " '10_cloverfield_lane(2016)',\n",
       " '10_things_i_hate_about_you(1999)',\n",
       " 'a_knights_tale(2001)',\n",
       " 'a_star_is_born(2018)',\n",
       " 'aladdin(1992)',\n",
       " 'all_dogs_go_to_heaven(1989)',\n",
       " 'americanbeauty1999.brrip',\n",
       " 'an_american_tail(1986)',\n",
       " \"angela's.christmas.2018.webrip.netflix\",\n",
       " 'angelas.christmas.wish.2020',\n",
       " 'babe(1995)',\n",
       " 'back_to_the_future(1985)',\n",
       " 'banking_on_bitcoin(2016)',\n",
       " 'batman_begins(2005)',\n",
       " 'beauty_and_the_beast(2017)',\n",
       " 'before_i_go_to_sleep(2014)',\n",
       " 'before_sunrise(1995)',\n",
       " 'before_sunset(2004)',\n",
       " 'braveheart(1995)',\n",
       " 'breaking_bad_the_movie(2017)',\n",
       " 'bren╨т.brown.the.call.to.courage.2019.720.nf..ddp.5.1.x264-cafeflix',\n",
       " 'bridget_jones_diary(2001)',\n",
       " 'bridget_joness_baby',\n",
       " 'cars(2006)',\n",
       " 'casper',\n",
       " 'cast_away(2000)',\n",
       " 'catch_me_if_you_can(2002)',\n",
       " 'charlie_and_the_chocolate_factory',\n",
       " 'cinderella(1950)',\n",
       " 'clueless(1995)',\n",
       " 'collateral.beauty.2016..brrip.x264.aac-etrg',\n",
       " 'crazy4tv.com - suits.s06e01..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e02..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e03..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e04..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e05..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e06..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e07..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e08..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e09..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e10..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e11..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e12..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e13..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e14..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e15..bluray.x265.hevc.crazy4ad',\n",
       " 'crazy4tv.com - suits.s06e16..bluray.x265.hevc.crazy4ad',\n",
       " 'crown, the s01e01 - wolferton splash.en',\n",
       " 'crown, the s01e01 - wolferton splash.en.sdh',\n",
       " 'crown, the s01e02 - hyde park corner.en',\n",
       " 'crown, the s01e02 - hyde park corner.en.sdh',\n",
       " 'crown, the s01e03 - windsor.en',\n",
       " 'crown, the s01e03 - windsor.en.forced',\n",
       " 'crown, the s01e03 - windsor.en.sdh',\n",
       " 'crown, the s01e04 - act of god.en',\n",
       " 'crown, the s01e04 - act of god.en.sdh',\n",
       " 'crown, the s01e05 - smoke and mirrors.en',\n",
       " 'crown, the s01e05 - smoke and mirrors.en.forced',\n",
       " 'crown, the s01e05 - smoke and mirrors.en.sdh',\n",
       " 'crown, the s01e06 - gelignite.en',\n",
       " 'crown, the s01e06 - gelignite.en.sdh',\n",
       " 'crown, the s01e07 - scientia potentia est.en',\n",
       " 'crown, the s01e07 - scientia potentia est.en.forced',\n",
       " 'crown, the s01e07 - scientia potentia est.en.sdh',\n",
       " 'crown, the s01e08 - pride & joy.en',\n",
       " 'crown, the s01e08 - pride & joy.en.sdh',\n",
       " 'crown, the s01e09 - assassins.en',\n",
       " 'crown, the s01e09 - assassins.en.sdh',\n",
       " 'crown, the s01e10 - gloriana.en',\n",
       " 'crown, the s01e10 - gloriana.en.forced',\n",
       " 'crown, the s01e10 - gloriana.en.sdh',\n",
       " 'deadpool(2016)',\n",
       " 'despicable_me(2010)',\n",
       " 'die_hard(1988)',\n",
       " 'downton abbey - s01e01 - episode 1.eng.sdh',\n",
       " 'downton abbey - s01e02 - episode 2.eng.sdh',\n",
       " 'downton abbey - s01e03 - episode 3.eng.sdh',\n",
       " 'downton abbey - s01e04 - episode 4.eng.sdh',\n",
       " 'downton abbey - s01e05 - episode 5.eng.sdh',\n",
       " 'downton abbey - s01e06 - episode 6.eng.sdh',\n",
       " 'downton abbey - s01e07 - episode 7.eng.sdh',\n",
       " 'dredd(2012)',\n",
       " 'dune(2021)',\n",
       " 'enola_holmes(2020)',\n",
       " 'entrapment',\n",
       " 'eurovision_song_contest_(2020)',\n",
       " 'ferdinand(2017)',\n",
       " 'fight_club(1999)',\n",
       " 'finding_nemo(2003)',\n",
       " 'forrest_gump(1994)',\n",
       " 'frozen.2013.web-dl.dsnp',\n",
       " 'ghosts.of.girlfriends.past.2009.bluray..x264.yify',\n",
       " 'gogo_loves_',\n",
       " 'good_will_hunting(1997)',\n",
       " 'groundhog_day(1993)',\n",
       " 'harry_potter_and_the_philosophers_stone(2001)',\n",
       " 'her(2013)',\n",
       " 'home_alone(1990)',\n",
       " 'hook(1991)',\n",
       " 'house_of_gucci(2021)',\n",
       " 'icarus.2017.web.x264-strife',\n",
       " 'indiana jones and the last crusade dvdrip xvid -izon-',\n",
       " 'inside_out(2015)',\n",
       " 'its_a_wonderful_life(1946)',\n",
       " 'klaus(2019)',\n",
       " 'knives_out(2019)',\n",
       " 'kubo_and_the_two_strings(2016)',\n",
       " 'liar_liar(1997)',\n",
       " 'lion(2016)',\n",
       " 'logan(2017)',\n",
       " 'love_actually(2003)',\n",
       " 'made_of_honor(2008)',\n",
       " 'mamma_mia(2008)',\n",
       " 'mary_poppins_returns(2018)',\n",
       " 'matilda(1996)',\n",
       " 'mechanic-resurrection_',\n",
       " 'meet_the_parents(2000)',\n",
       " 'men.in.black.1997..bluray.x264-septic',\n",
       " 'milada(2017)',\n",
       " 'mona_lisa_smile(2003)',\n",
       " 'moulin_rouge(2001)',\n",
       " 'mrs_doubtfire(1993)',\n",
       " 'my_big_fat_greek_wedding(2002)',\n",
       " 'notting_hill(1999)',\n",
       " 'oceans_eleven(2001)',\n",
       " 'oceans_twelve(2004)',\n",
       " 'pirates_of_the_caribbean(2003)',\n",
       " 'pleasantville(1998)',\n",
       " 'powder(1995)',\n",
       " 'pride_and_prejudice',\n",
       " 'pulp_fiction(1994)',\n",
       " 'rat.race.2001.1080p.web-dl.dd5.1.h264-fgt',\n",
       " 'ratatouille(2007)',\n",
       " 'ready_or_not(2019)',\n",
       " 'roman holiday(1953)',\n",
       " \"secrets of her majesty's secret service eng\",\n",
       " 'seven.worlds.one.planet.s01e01.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e02.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e03.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e04.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e05.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e06.2160p.bluray.remux.eng',\n",
       " 'seven.worlds.one.planet.s01e07.2160p.bluray.remux.eng',\n",
       " 'shrek(2001)',\n",
       " 'sleepless_in_seattle(1993)',\n",
       " 'slingshot (2014) web.eng',\n",
       " 'somm.into.the.bottle.2015.1080p.bluray.x265-rarbg.en',\n",
       " 'soul(2020)',\n",
       " 'spirit.stallion.of.the.cimarron.en',\n",
       " 'suits s04e01',\n",
       " 'suits s04e02',\n",
       " 'suits s04e03',\n",
       " 'suits s04e04',\n",
       " 'suits s04e05',\n",
       " 'suits s04e06',\n",
       " 'suits s04e07',\n",
       " 'suits s04e08',\n",
       " 'suits s04e09',\n",
       " 'suits s04e10',\n",
       " 'suits s04e11',\n",
       " 'suits s04e12',\n",
       " 'suits s04e13',\n",
       " 'suits s04e14',\n",
       " 'suits s04e15',\n",
       " 'suits s04e16',\n",
       " 'suits.episode 1- denial',\n",
       " 'suits.episode 10- faith',\n",
       " 'suits.episode 11- blowback',\n",
       " 'suits.episode 12- live to fight',\n",
       " \"suits.episode 13- god's green earth\",\n",
       " 'suits.episode 14- self defense',\n",
       " 'suits.episode 15- tick tock',\n",
       " 'suits.episode 16- 25th hour',\n",
       " 'suits.episode 2- compensation',\n",
       " 'suits.episode 3- no refills',\n",
       " 'suits.episode 4- no puedo hacerlo',\n",
       " 'suits.episode 5- toe to toe',\n",
       " 'suits.episode 6- privilege',\n",
       " 'suits.episode 7- hitting home',\n",
       " 'suits.episode 8- mea culpa',\n",
       " 'suits.episode 9- uninvited guests',\n",
       " 'suits.s01e01.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e02.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e03.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e04.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e05.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e06.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e07.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e08.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e09.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e10.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e11.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s01e12.1080p.bluray.aac5.1.x265-dtg.02.en',\n",
       " 'suits.s02e01.hdtv.x264-',\n",
       " 'suits.s02e02.hdtv.x264-',\n",
       " 'suits.s02e03.hdtv.x264-',\n",
       " 'suits.s02e04.hdtv.x264-',\n",
       " 'suits.s02e05..hdtv.x264-',\n",
       " 'suits.s02e06.all.in.hdtv.x264-',\n",
       " 'suits.s02e07.sucker.punch.proper.hdtv.x264-',\n",
       " 'suits.s02e08.hdtv.x264-',\n",
       " 'suits.s02e09.hdtv.x264-',\n",
       " 'suits.s02e10.hdtv.x264-',\n",
       " 'suits.s02e11.hdtv.x264-',\n",
       " 'suits.s02e12.hdtv.x264-',\n",
       " 'suits.s02e13..hdtv.x264-',\n",
       " 'suits.s02e14.hdtv.x264-',\n",
       " 'suits.s02e15.hdtv.x264-',\n",
       " 'suits.s02e16.hdtv.x264-',\n",
       " 'suits.s03e01.hdtv.x264-',\n",
       " 'suits.s03e02.hdtv.x264-',\n",
       " 'suits.s03e03.hdtv.x264-',\n",
       " 'suits.s03e04.hdtv.x264-',\n",
       " 'suits.s03e05..hdtv.x264-',\n",
       " 'suits.s03e06..hdtv.x264-',\n",
       " 'suits.s03e07.hdtv.x264-',\n",
       " 'suits.s03e08..hdtv.x264-',\n",
       " 'suits.s03e09..hdtv.x264-',\n",
       " 'suits.s03e10.hdtv.x264-',\n",
       " 'terminator_2_judgment_day_1991_rony',\n",
       " 'the walking dead-s01e01-days gone bye.',\n",
       " 'the walking dead-s01e02-guts.',\n",
       " 'the walking dead-s01e03-tell it to the frogs.',\n",
       " 'the walking dead-s01e04-vatos.',\n",
       " 'the walking dead-s01e05-wildfire.',\n",
       " 'the walking dead-s01e06-ts-19.',\n",
       " 'the.grinch.2018.remux.1080p.blu-ray.avc.truehd.dts-hd.ma.7.1-legi0n.',\n",
       " 'the.notebook.dvdrip.xvid-diamond',\n",
       " 'the.sound.of.music.1965.webrip.itunes',\n",
       " 'the.true.cost.2015.bluray..700mb.ganool.com',\n",
       " 'the_blind_side(2009)',\n",
       " 'the_break-up(2006)',\n",
       " 'the_cabin_in_the_woods(2012)',\n",
       " 'the_devil_wears_prad',\n",
       " 'the_fault_in_our_stars(2014)',\n",
       " 'the_fundamentals_of_caring(2016)',\n",
       " 'the_ghost_writer',\n",
       " 'the_graduate(1967)',\n",
       " 'the_greatest_showman(2017)',\n",
       " 'the_hangover(2009)',\n",
       " 'the_holiday(2006)',\n",
       " 'the_intern(2015)',\n",
       " 'the_invisible_man(2020)',\n",
       " 'the_jungle_book(2016)',\n",
       " 'the_kings_speech(2010)',\n",
       " 'the_legend_of_tarzan(2016)',\n",
       " 'the_lion_king(1994)',\n",
       " 'the_lord_of_the_rings(2001)',\n",
       " 'the_man_called_flintstone(1966)',\n",
       " 'the_secret_life_of_walter_mitty(2013)',\n",
       " 'the_shawshank_redemption(1994)',\n",
       " 'the_social_network(2010)',\n",
       " 'the_terminal(2004)',\n",
       " 'the_terminator(1984)',\n",
       " 'the_theory_of_everything(2014)',\n",
       " 'the_usual_suspects(1995)',\n",
       " 'titanic(1997)',\n",
       " 'toy_story(1995)',\n",
       " 'twilight(2008)',\n",
       " 'up(2009)',\n",
       " \"valentine's.day.2010.subtitles.yify\",\n",
       " 'venom(2018)',\n",
       " 'virgin.river.s01e01.internal..web.x264-strife',\n",
       " 'virgin.river.s01e02.internal..web.x264-strife',\n",
       " 'virgin.river.s01e03.internal..web.x264-strife',\n",
       " 'virgin.river.s01e04.internal..web.x264-strife',\n",
       " 'virgin.river.s01e05.internal..web.x264-strife',\n",
       " 'virgin.river.s01e06.internal..web.x264-strife',\n",
       " 'virgin.river.s01e07.internal..web.x264-strife',\n",
       " 'virgin.river.s01e08.internal..web.x264-strife',\n",
       " 'virgin.river.s01e09.internal..web.x264-strife',\n",
       " 'virgin.river.s01e10.internal..web.x264-strife',\n",
       " 'warm_bodies(2013)',\n",
       " 'we_are_the_millers(2013)',\n",
       " 'westworld_scenes_of_dr_robert_ford',\n",
       " 'while_you_were_sleeping(1995)',\n",
       " 'z srt23 uk-bun gullivers.travels.1939..bluray.x264-cinefile',\n",
       " 'zootopia(2016)']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data['movie'] = data['movie'].replace(space_replace, regex=False)\n",
    "#movies_labels['movie'] = movies_labels['movie'].str.strip()\n",
    "data['movie'] = data['movie'].replace(space_replace_1, regex=True)\n",
    "#movies_labels['movie'] = movies_labels['movie'].str.strip()\n",
    "data['movie'] = data['movie'].replace(space_replace_3, regex=True)\n",
    "data['movie'] = data['movie'].str.strip()\n",
    "data['movie'] = data['movie'].str.lower()\n",
    "sorted(data['movie'].unique())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39114230",
   "metadata": {},
   "source": [
    "**ОБЪЕДИНИМ ДВА ДАТАСЕТА, РАССМОТРЕННЫХ РАНЕЕ В ОДИН:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "322ee6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 464 entries, 0 to 463\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   movie      464 non-null    object\n",
      " 1   subtitels  279 non-null    object\n",
      " 2   level      464 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 14.5+ KB\n"
     ]
    }
   ],
   "source": [
    "dfs = [data, movies_labels]\n",
    "#merge all DataFrames into one\n",
    "final_df = reduce(lambda left,right: pd.merge(left,right,on=['movie','level'],\n",
    " how='outer'), dfs)#.fillna('none')\n",
    "final_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a068d5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the walking dead-s01e01-days gone bye.</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the walking dead-s01e02-guts.</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the walking dead-s01e03-tell it to the frogs.</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the walking dead-s01e04-vatos.</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the walking dead-s01e05-wildfire.</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>suits.episode 8- mea culpa</td>\n",
       "      <td>so we started this thing i was just doing it t...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>suits.episode 9- uninvited guests</td>\n",
       "      <td>pursuant to section b of the bylaws i am placi...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>suits.s01e01.1080p.bluray.aac5.1.x265-dtg.02.en</td>\n",
       "      <td>gerald tate's here he wants to know what's hap...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>suits.s01e02.1080p.bluray.aac5.1.x265-dtg.02.en</td>\n",
       "      <td>uh what is that three in a row that would be f...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>suits.s01e03.1080p.bluray.aac5.1.x265-dtg.02.en</td>\n",
       "      <td>the tesla roadster sport offered only to our m...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              movie  \\\n",
       "0            the walking dead-s01e01-days gone bye.   \n",
       "1                     the walking dead-s01e02-guts.   \n",
       "2     the walking dead-s01e03-tell it to the frogs.   \n",
       "3                    the walking dead-s01e04-vatos.   \n",
       "4                 the walking dead-s01e05-wildfire.   \n",
       "..                                              ...   \n",
       "85                       suits.episode 8- mea culpa   \n",
       "86                suits.episode 9- uninvited guests   \n",
       "87  suits.s01e01.1080p.bluray.aac5.1.x265-dtg.02.en   \n",
       "88  suits.s01e02.1080p.bluray.aac5.1.x265-dtg.02.en   \n",
       "89  suits.s01e03.1080p.bluray.aac5.1.x265-dtg.02.en   \n",
       "\n",
       "                                            subtitels level  \n",
       "0           little girl i'm a policeman little gir...    A2  \n",
       "1    mom right here  any luck how do we tell if th...    A2  \n",
       "2    that's right you heard me bitch you got a pro...    A2  \n",
       "3    what nothing it's not nothing it's always som...    A2  \n",
       "4   walkie talkie squawks morgan i don't know if y...    A2  \n",
       "..                                                ...   ...  \n",
       "85  so we started this thing i was just doing it t...    B2  \n",
       "86  pursuant to section b of the bylaws i am placi...    B2  \n",
       "87  gerald tate's here he wants to know what's hap...    B2  \n",
       "88  uh what is that three in a row that would be f...    B2  \n",
       "89  the tesla roadster sport offered only to our m...    B2  \n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839929d",
   "metadata": {},
   "source": [
    "Обработаем полученные данные в столбцах:\n",
    " - уберем в колонке level значение, не являющее вровнем (Subtitles)\n",
    " - удалим строки с NAN в колонке subtitels\n",
    " - проверем колонку movie на наличие скрытых дубликатов, приведя наименования немного в порядок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73c8b5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A2', 'B1', 'B2', 'C1', 'Subtitles']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# смотрим уникальные значения колонки level:\n",
    "sorted(final_df['level'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "096ccb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2           176\n",
       "Subtitles    116\n",
       "B1            68\n",
       "C1            66\n",
       "A2            38\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "698c21e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2    176\n",
       "B1     68\n",
       "C1     66\n",
       "A2     38\n",
       "Name: level, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удаляем фильмы, уровень которых опеределен как Subtitles\n",
    "final_df = final_df[final_df['level'] != \"Subtitles\"]\n",
    "final_df['level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "25525651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>10_cloverfield_lane(2016)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>10_things_i_hate_about_you(1999)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>a_knights_tale(2001)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>a_star_is_born(2018)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>aladdin(1992)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>matilda(2022)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>bullet train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>thor: love and thunder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>lightyear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>the grinch</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                movie subtitels level\n",
       "279         10_cloverfield_lane(2016)       NaN    B1\n",
       "280  10_things_i_hate_about_you(1999)       NaN    B1\n",
       "281              a_knights_tale(2001)       NaN    B2\n",
       "282              a_star_is_born(2018)       NaN    B2\n",
       "283                     aladdin(1992)       NaN    A2\n",
       "..                                ...       ...   ...\n",
       "459                     matilda(2022)       NaN    C1\n",
       "460                      bullet train       NaN    B1\n",
       "461            thor: love and thunder       NaN    B2\n",
       "462                         lightyear       NaN    B2\n",
       "463                        the grinch       NaN    B1\n",
       "\n",
       "[185 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Рассмотрим строки в котрых отсутсвуют субтитры.\n",
    "final_df.loc[final_df['subtitels'].isna() ==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "325b9a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 163 entries, 0 to 162\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   movie      163 non-null    object\n",
      " 1   subtitels  163 non-null    object\n",
      " 2   level      163 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#удаляем строки с отсутствующими субтитрами и смотрим на оствшийся датасет\n",
    "#final_df = final_df.dropna(axis='index', how='any', subset=['subtitels']).reset_index(drop=True)\n",
    "final_df = final_df.dropna(axis='index', subset=['subtitels']).reset_index(drop=True)\n",
    "final_df.head(20)\n",
    "final_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916084d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# рассмотри колонку с наименование фильмов и слегка приведем ее в порядок:\n",
    "sorted(final_df['movie'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe39f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим список замены других возможных комбинаций пробелов и дефисов:\n",
    "space_replace = {' _ ':' ','_ ':' ',',' :' ',' _':' ', ' - ':' ', '-':' ',r'\\.':' ',r'\\ ':' ','_':' ','╨':' ','+':'','/':' ','╨т':''}\n",
    "df_final['movie'] = df_final['movie'].replace(space_replace, regex=False)\n",
    "sorted(final_df['movie'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80db5b1",
   "metadata": {},
   "source": [
    "**СЛОВАРИ**\n",
    "\n",
    "Заказчиком было предоставлено несколько словарей с разбиением слов по уровнем сложности английского языка.\n",
    "Предварительно словари были переведены из pdf формата в exel И объединены.\n",
    "\n",
    "Считаем полученный словарь и сформируем из него датасет.\n",
    "\n",
    "У нас по два словаря:\n",
    "Американский на 3000 слов и на 5000 слов \n",
    "Оксфордский на 3000 слов и на 5000 слов\n",
    "\n",
    "Сделаем замену наименований словарей на просто USA и Oxford\n",
    "Отсортируем данные по кадому из словарей и найдем дубликаты.\n",
    "Будем оставлять каждый первый (ориентируемся, что если слово есть и в легком и в сложном уровне, то скорее всего студент его уже знает и изучил все семантические значения, как только узнал слово, т.е. с первого уровня.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "338770f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>word</th>\n",
       "      <th>comment</th>\n",
       "      <th>semantic</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>an</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1</td>\n",
       "      <td>about</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>above</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1</td>\n",
       "      <td>across</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  level    word comment            semantic  \\\n",
       "0    A1       a     NaN  indefinite article   \n",
       "1    A1      an     NaN  indefinite article   \n",
       "2    A1   about     NaN         prep., adv.   \n",
       "3    A1   above     NaN         prep., adv.   \n",
       "4    A1  across     NaN         prep., adv.   \n",
       "\n",
       "                                     file  \n",
       "0  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "1  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "2  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "3  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "4  American_Oxford_3000_by_CEFR_level.pdf  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# считываем словарь и создаём дополнительный объект\n",
    "df_words = pd.read_excel(DICTIONARY)\n",
    "#df_words = df_words.groupby(['level', 'word'])['file'].count().reset_index()\n",
    "df_words.head()\n",
    "\n",
    "#dict_words = {}\n",
    "\n",
    "#for level in df_words['level'].unique():\n",
    "    # создаём новые колонки\n",
    "#    df_subtitles[level] = 0\n",
    "    \n",
    "#    dict_words[level] = df_words.loc[df_words['level'] == level, 'word'].values\n",
    "    \n",
    "#print(f'Были созданы колонки {\", \".join(df_words[\"level\"].unique())}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6162e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>word</th>\n",
       "      <th>comment</th>\n",
       "      <th>semantic</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>an</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1</td>\n",
       "      <td>about</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>above</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1</td>\n",
       "      <td>across</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>American_Oxford_3000_by_CEFR_level.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  level    word comment            semantic  \\\n",
       "0    A1       a     NaN  indefinite article   \n",
       "1    A1      an     NaN  indefinite article   \n",
       "2    A1   about     NaN         prep., adv.   \n",
       "3    A1   above     NaN         prep., adv.   \n",
       "4    A1  across     NaN         prep., adv.   \n",
       "\n",
       "                                     file  \n",
       "0  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "1  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "2  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "3  American_Oxford_3000_by_CEFR_level.pdf  \n",
       "4  American_Oxford_3000_by_CEFR_level.pdf  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "503cdd00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Oxford', 'USA']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_replace = {'American_Oxford_3000_by_CEFR_level.pdf':'USA', 'American_Oxford_5000_by_CEFR_level.pdf':'USA', 'The_Oxford_3000_by_CEFR_level.pdf':'Oxford',\n",
    "'The_Oxford_5000_by_CEFR_level.pdf':'Oxford'}\n",
    "df_words['file'] = df_words['file'].replace(file_replace, regex=True)\n",
    "sorted(df_words['file'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81143579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level</th>\n",
       "      <th>word</th>\n",
       "      <th>comment</th>\n",
       "      <th>semantic</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A1</td>\n",
       "      <td>a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A1</td>\n",
       "      <td>an</td>\n",
       "      <td>NaN</td>\n",
       "      <td>indefinite article</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1</td>\n",
       "      <td>about</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A1</td>\n",
       "      <td>above</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A1</td>\n",
       "      <td>across</td>\n",
       "      <td>NaN</td>\n",
       "      <td>prep., adv.</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  level    word comment            semantic file\n",
       "0    A1       a     NaN  indefinite article  USA\n",
       "1    A1      an     NaN  indefinite article  USA\n",
       "2    A1   about     NaN         prep., adv.  USA\n",
       "3    A1   above     NaN         prep., adv.  USA\n",
       "4    A1  across     NaN         prep., adv.  USA"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5eca416e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1</th>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2</th>\n",
       "      <td>1471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word\n",
       "level      \n",
       "A1      761\n",
       "A2      767\n",
       "B1      775\n",
       "B2     1471\n",
       "C1     1455"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort words and keep unique with higher levels\n",
    "df_words = df_words.sort_values(by=['word', 'level'], ascending=True)\n",
    "df_words = df_words.drop_duplicates(subset=['word'], keep='last')# сохраняем вариант Оксофрдского словаря из дуюликатов\n",
    "df_words.head()\n",
    "df_words.groupby('level').agg({'word':'count'}) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fd87896",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"C:/Users/zst/Desktop/jupiter_projects/мастерская_2/df_words\",\"wb\") as fid_0: \n",
    "    dump(df_words,fid_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4edcedbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Нормализация субтитров основного рабочего файла:\n",
    "# Проведем токенизацию субтитров\n",
    "# далее уберем все стоп-слова\n",
    "# Проведем лемантизацию и далее найдем частоту"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45245286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_prep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the walking dead-s01e01-days gone bye.</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[little, girl, i, a, policeman, little, girl, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the walking dead-s01e02-guts.</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[mom, right, here, any, luck, how, do, we, tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the walking dead-s01e03-tell it to the frogs.</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[that, right, you, heard, me, bitch, you, got,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the walking dead-s01e04-vatos.</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[what, nothing, it, not, nothing, it, always, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the walking dead-s01e05-wildfire.</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, i, do, know,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           movie  \\\n",
       "0         the walking dead-s01e01-days gone bye.   \n",
       "1                  the walking dead-s01e02-guts.   \n",
       "2  the walking dead-s01e03-tell it to the frogs.   \n",
       "3                 the walking dead-s01e04-vatos.   \n",
       "4              the walking dead-s01e05-wildfire.   \n",
       "\n",
       "                                           subtitels level  \\\n",
       "0          little girl i'm a policeman little gir...    A2   \n",
       "1   mom right here  any luck how do we tell if th...    A2   \n",
       "2   that's right you heard me bitch you got a pro...    A2   \n",
       "3   what nothing it's not nothing it's always som...    A2   \n",
       "4  walkie talkie squawks morgan i don't know if y...    A2   \n",
       "\n",
       "                                           subs_prep  \n",
       "0  [little, girl, i, a, policeman, little, girl, ...  \n",
       "1  [mom, right, here, any, luck, how, do, we, tel...  \n",
       "2  [that, right, you, heard, me, bitch, you, got,...  \n",
       "3  [what, nothing, it, not, nothing, it, always, ...  \n",
       "4  [walkie, talkie, squawks, morgan, i, do, know,...  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_df['text_prep'] = final_df['subtitels'].apply(nltk.word_tokenize)\n",
    "#words = word_tokenize(final_df['subtitles'])\n",
    "english_stopwords = stopwords.words('english')\n",
    "#def remove_numbers(text):\n",
    " #   return ''.join([i if not i.isdigit() else ' ' for i in text])\n",
    "\n",
    "def tokenize(column):    \n",
    "    tokens = nltk.word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()] \n",
    "\n",
    "prep_text = [tokenize(text) for text in final_df['subtitels'].astype('str')]\n",
    "#final_df['tokenized'] = final_df.apply(lambda x: tokenize([remove_numbers(text) for text in x['subtitels'].astype('str')]), axis=1)\n",
    "final_df['subs_prep'] = prep_text\n",
    "final_df.head()\n",
    "\n",
    "#def token_stopwords(text):\n",
    "#    text_tokens = row['text']\n",
    "#    text_tokens = word_tokenize([remove_numbers(text) for text in row['text']])#.astype('str')#]) \n",
    "#    tokens_without_sw = [word for word in text_tokens if not word in english_stopword] \n",
    "#    return tokens_without_sw\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ca0a1a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_stopwords(text):\n",
    "    tokens_without_sw = [word for word in text if not word in english_stopwords] \n",
    "    return tokens_without_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6acdffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_prep</th>\n",
       "      <th>subs_nomal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the walking dead-s01e01-days gone bye.</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[little, girl, i, a, policeman, little, girl, ...</td>\n",
       "      <td>[little, girl, policeman, little, girl, afraid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the walking dead-s01e02-guts.</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[mom, right, here, any, luck, how, do, we, tel...</td>\n",
       "      <td>[mom, right, luck, tell, poison, uh, one, sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the walking dead-s01e03-tell it to the frogs.</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[that, right, you, heard, me, bitch, you, got,...</td>\n",
       "      <td>[right, heard, bitch, got, problem, bring, man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the walking dead-s01e04-vatos.</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[what, nothing, it, not, nothing, it, always, ...</td>\n",
       "      <td>[nothing, nothing, always, something, dad, tea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the walking dead-s01e05-wildfire.</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, i, do, know,...</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, know, know, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           movie  \\\n",
       "0         the walking dead-s01e01-days gone bye.   \n",
       "1                  the walking dead-s01e02-guts.   \n",
       "2  the walking dead-s01e03-tell it to the frogs.   \n",
       "3                 the walking dead-s01e04-vatos.   \n",
       "4              the walking dead-s01e05-wildfire.   \n",
       "\n",
       "                                           subtitels level  \\\n",
       "0          little girl i'm a policeman little gir...    A2   \n",
       "1   mom right here  any luck how do we tell if th...    A2   \n",
       "2   that's right you heard me bitch you got a pro...    A2   \n",
       "3   what nothing it's not nothing it's always som...    A2   \n",
       "4  walkie talkie squawks morgan i don't know if y...    A2   \n",
       "\n",
       "                                           subs_prep  \\\n",
       "0  [little, girl, i, a, policeman, little, girl, ...   \n",
       "1  [mom, right, here, any, luck, how, do, we, tel...   \n",
       "2  [that, right, you, heard, me, bitch, you, got,...   \n",
       "3  [what, nothing, it, not, nothing, it, always, ...   \n",
       "4  [walkie, talkie, squawks, morgan, i, do, know,...   \n",
       "\n",
       "                                          subs_nomal  \n",
       "0  [little, girl, policeman, little, girl, afraid...  \n",
       "1  [mom, right, luck, tell, poison, uh, one, sure...  \n",
       "2  [right, heard, bitch, got, problem, bring, man...  \n",
       "3  [nothing, nothing, always, something, dad, tea...  \n",
       "4  [walkie, talkie, squawks, morgan, know, know, ...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выполняем нормализацию данных\n",
    "final_df['subs_nomal'] = final_df['subs_prep'].apply(token_stopwords)\n",
    "\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17c25849",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "#importing stop words from English language.\n",
    "#spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fbed5557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_prep</th>\n",
       "      <th>subs_nomal</th>\n",
       "      <th>B2</th>\n",
       "      <th>A1</th>\n",
       "      <th>B1</th>\n",
       "      <th>A2</th>\n",
       "      <th>C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the walking dead-s01e01-days gone bye.</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[little, girl, i, a, policeman, little, girl, ...</td>\n",
       "      <td>[little, girl, policeman, little, girl, afraid...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the walking dead-s01e02-guts.</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[mom, right, here, any, luck, how, do, we, tel...</td>\n",
       "      <td>[mom, right, luck, tell, poison, uh, one, sure...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the walking dead-s01e03-tell it to the frogs.</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[that, right, you, heard, me, bitch, you, got,...</td>\n",
       "      <td>[right, heard, bitch, got, problem, bring, man...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the walking dead-s01e04-vatos.</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[what, nothing, it, not, nothing, it, always, ...</td>\n",
       "      <td>[nothing, nothing, always, something, dad, tea...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the walking dead-s01e05-wildfire.</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, i, do, know,...</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, know, know, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           movie  \\\n",
       "0         the walking dead-s01e01-days gone bye.   \n",
       "1                  the walking dead-s01e02-guts.   \n",
       "2  the walking dead-s01e03-tell it to the frogs.   \n",
       "3                 the walking dead-s01e04-vatos.   \n",
       "4              the walking dead-s01e05-wildfire.   \n",
       "\n",
       "                                           subtitels level  \\\n",
       "0          little girl i'm a policeman little gir...    A2   \n",
       "1   mom right here  any luck how do we tell if th...    A2   \n",
       "2   that's right you heard me bitch you got a pro...    A2   \n",
       "3   what nothing it's not nothing it's always som...    A2   \n",
       "4  walkie talkie squawks morgan i don't know if y...    A2   \n",
       "\n",
       "                                           subs_prep  \\\n",
       "0  [little, girl, i, a, policeman, little, girl, ...   \n",
       "1  [mom, right, here, any, luck, how, do, we, tel...   \n",
       "2  [that, right, you, heard, me, bitch, you, got,...   \n",
       "3  [what, nothing, it, not, nothing, it, always, ...   \n",
       "4  [walkie, talkie, squawks, morgan, i, do, know,...   \n",
       "\n",
       "                                          subs_nomal  B2  A1  B1  A2  C1  \n",
       "0  [little, girl, policeman, little, girl, afraid...   0   0   0   0   0  \n",
       "1  [mom, right, luck, tell, poison, uh, one, sure...   0   0   0   0   0  \n",
       "2  [right, heard, bitch, got, problem, bring, man...   0   0   0   0   0  \n",
       "3  [nothing, nothing, always, something, dad, tea...   0   0   0   0   0  \n",
       "4  [walkie, talkie, squawks, morgan, know, know, ...   0   0   0   0   0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_words = {}\n",
    "# создаём новые колонки по уровням в основном датасете:\n",
    "for level in df_words['level'].unique():\n",
    "    final_df[level] = 0\n",
    "    \n",
    "    dict_words[level] = df_words.loc[df_words['level'] == level, 'word'].values\n",
    "    \n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "07211e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем доли слов определённых категорий в фильме, используя словарь с указанием уровня:\n",
    "def level_words(row):     \n",
    "    words = row['subs_lemm']\n",
    "        \n",
    "    for level in df_words['level'].unique():\n",
    "        row[level] = len([word for word in words if word.lower() in dict_words[level]]) / len(words)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cdc96cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subtitels</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_prep</th>\n",
       "      <th>subs_nomal</th>\n",
       "      <th>B2</th>\n",
       "      <th>A1</th>\n",
       "      <th>B1</th>\n",
       "      <th>A2</th>\n",
       "      <th>C1</th>\n",
       "      <th>subs_lemm</th>\n",
       "      <th>sub_for_ml</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the walking dead-s01e01-days gone bye.</td>\n",
       "      <td>little girl i'm a policeman little gir...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[little, girl, i, a, policeman, little, girl, ...</td>\n",
       "      <td>[little, girl, policeman, little, girl, afraid...</td>\n",
       "      <td>0.077070</td>\n",
       "      <td>0.382166</td>\n",
       "      <td>0.065605</td>\n",
       "      <td>0.141401</td>\n",
       "      <td>0.029299</td>\n",
       "      <td>[little, girl, policeman, little, girl, afraid...</td>\n",
       "      <td>little girl policeman little girl afraid okay ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the walking dead-s01e02-guts.</td>\n",
       "      <td>mom right here  any luck how do we tell if th...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[mom, right, here, any, luck, how, do, we, tel...</td>\n",
       "      <td>[mom, right, luck, tell, poison, uh, one, sure...</td>\n",
       "      <td>0.087417</td>\n",
       "      <td>0.364901</td>\n",
       "      <td>0.080795</td>\n",
       "      <td>0.123841</td>\n",
       "      <td>0.022517</td>\n",
       "      <td>[mom, right, luck, tell, poison, uh, one, sure...</td>\n",
       "      <td>mom right luck tell poison uh one sure way kno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the walking dead-s01e03-tell it to the frogs.</td>\n",
       "      <td>that's right you heard me bitch you got a pro...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[that, right, you, heard, me, bitch, you, got,...</td>\n",
       "      <td>[right, heard, bitch, got, problem, bring, man...</td>\n",
       "      <td>0.080671</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.053955</td>\n",
       "      <td>0.122053</td>\n",
       "      <td>0.029859</td>\n",
       "      <td>[right, heard, bitch, got, problem, bring, man...</td>\n",
       "      <td>right heard bitch got problem bring man enough...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the walking dead-s01e04-vatos.</td>\n",
       "      <td>what nothing it's not nothing it's always som...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[what, nothing, it, not, nothing, it, always, ...</td>\n",
       "      <td>[nothing, nothing, always, something, dad, tea...</td>\n",
       "      <td>0.079605</td>\n",
       "      <td>0.350959</td>\n",
       "      <td>0.073213</td>\n",
       "      <td>0.120860</td>\n",
       "      <td>0.017432</td>\n",
       "      <td>[nothing, nothing, always, something, dad, tea...</td>\n",
       "      <td>nothing nothing always something dad teach tie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the walking dead-s01e05-wildfire.</td>\n",
       "      <td>walkie talkie squawks morgan i don't know if y...</td>\n",
       "      <td>A2</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, i, do, know,...</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, know, know, ...</td>\n",
       "      <td>0.080634</td>\n",
       "      <td>0.369330</td>\n",
       "      <td>0.092153</td>\n",
       "      <td>0.125990</td>\n",
       "      <td>0.026638</td>\n",
       "      <td>[walkie, talkie, squawks, morgan, know, know, ...</td>\n",
       "      <td>walkie talkie squawks morgan know know hear ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           movie  \\\n",
       "0         the walking dead-s01e01-days gone bye.   \n",
       "1                  the walking dead-s01e02-guts.   \n",
       "2  the walking dead-s01e03-tell it to the frogs.   \n",
       "3                 the walking dead-s01e04-vatos.   \n",
       "4              the walking dead-s01e05-wildfire.   \n",
       "\n",
       "                                           subtitels level  \\\n",
       "0          little girl i'm a policeman little gir...    A2   \n",
       "1   mom right here  any luck how do we tell if th...    A2   \n",
       "2   that's right you heard me bitch you got a pro...    A2   \n",
       "3   what nothing it's not nothing it's always som...    A2   \n",
       "4  walkie talkie squawks morgan i don't know if y...    A2   \n",
       "\n",
       "                                           subs_prep  \\\n",
       "0  [little, girl, i, a, policeman, little, girl, ...   \n",
       "1  [mom, right, here, any, luck, how, do, we, tel...   \n",
       "2  [that, right, you, heard, me, bitch, you, got,...   \n",
       "3  [what, nothing, it, not, nothing, it, always, ...   \n",
       "4  [walkie, talkie, squawks, morgan, i, do, know,...   \n",
       "\n",
       "                                          subs_nomal        B2        A1  \\\n",
       "0  [little, girl, policeman, little, girl, afraid...  0.077070  0.382166   \n",
       "1  [mom, right, luck, tell, poison, uh, one, sure...  0.087417  0.364901   \n",
       "2  [right, heard, bitch, got, problem, bring, man...  0.080671  0.356207   \n",
       "3  [nothing, nothing, always, something, dad, tea...  0.079605  0.350959   \n",
       "4  [walkie, talkie, squawks, morgan, know, know, ...  0.080634  0.369330   \n",
       "\n",
       "         B1        A2        C1  \\\n",
       "0  0.065605  0.141401  0.029299   \n",
       "1  0.080795  0.123841  0.022517   \n",
       "2  0.053955  0.122053  0.029859   \n",
       "3  0.073213  0.120860  0.017432   \n",
       "4  0.092153  0.125990  0.026638   \n",
       "\n",
       "                                           subs_lemm  \\\n",
       "0  [little, girl, policeman, little, girl, afraid...   \n",
       "1  [mom, right, luck, tell, poison, uh, one, sure...   \n",
       "2  [right, heard, bitch, got, problem, bring, man...   \n",
       "3  [nothing, nothing, always, something, dad, tea...   \n",
       "4  [walkie, talkie, squawks, morgan, know, know, ...   \n",
       "\n",
       "                                          sub_for_ml  \n",
       "0  little girl policeman little girl afraid okay ...  \n",
       "1  mom right luck tell poison uh one sure way kno...  \n",
       "2  right heard bitch got problem bring man enough...  \n",
       "3  nothing nothing always something dad teach tie...  \n",
       "4  walkie talkie squawks morgan know know hear ma...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лемантизация:\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "lemm_texts_list_1 =[]\n",
    "lemm_texts_list_2 = []\n",
    "for text in final_df['subs_nomal']:\n",
    "    text_lem = [morph.parse(word)[0].normal_form for word in text]\n",
    "    if len(text_lem) <= 1:\n",
    "        lemm_texts_list_1.append('')\n",
    "        lemm_texts_list_2.append('')\n",
    "        continue\n",
    "    lemm_texts_list_1.append(text_lem)\n",
    "    lemm_texts_list_2.append(' '.join(text_lem))\n",
    "final_df['subs_lemm']= lemm_texts_list_1\n",
    "final_df = final_df[final_df['subs_lemm']!='']\n",
    "final_df = final_df.apply(level_words, axis=1)\n",
    "final_df['sub_for_ml']= lemm_texts_list_2\n",
    "final_df = final_df[final_df['sub_for_ml']!='']\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7d2f44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndict_words = {}\\n# создаём новые колонки по уровням в основном датасете:\\nfor level in ENGLISH_LEVELS:\\n    final_df[level] = 0\\n    \\n    dict_words[level] = df_words.loc[df_words['level'] == level, 'word'].values\\n    \\nfinal_df.head()\\n\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dict_words = {}\n",
    "# создаём новые колонки по уровням в основном датасете:\n",
    "for level in ENGLISH_LEVELS:\n",
    "    final_df[level] = 0\n",
    "    \n",
    "    dict_words[level] = df_words.loc[df_words['level'] == level, 'word'].values\n",
    "    \n",
    "final_df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "87fa5796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Устанавливаем доли слов определённых категорий в фильме, используя словарь с указанием уровня:\\ndef level_words(row):     \\n    words = row['subs_lemm']\\n        \\n    for level in df_words['level'].unique():\\n        row[level] = len([word for word in words if word.lower() in dict_words[level]]) / len(words)\\n    return row\\n\\nfinal_df = final_df.apply(level_words, axis=1)\\nfinal_df.head()\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Устанавливаем доли слов определённых категорий в фильме, используя словарь с указанием уровня:\n",
    "def level_words(row):     \n",
    "    words = row['subs_lemm']\n",
    "        \n",
    "    for level in df_words['level'].unique():\n",
    "        row[level] = len([word for word in words if word.lower() in dict_words[level]]) / len(words)\n",
    "    return row\n",
    "\n",
    "final_df = final_df.apply(level_words, axis=1)\n",
    "final_df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e674bf",
   "metadata": {},
   "source": [
    "**ПОДГОТОВКА ДАННЫХ ДЛЯ ОБУЧЕНИЯ МОДЕЛЕЙ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46a1ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: 122\n",
      "Валидационная выборка: 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71     B2\n",
       "104    B2\n",
       "68     B2\n",
       "154    C1\n",
       "120    B2\n",
       "Name: level, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_df['sub_for_ml'] = final_df['sub_for_ml'].astype('str')\n",
    "features = final_df[['sub_for_ml', 'A1', 'A2', 'B1', 'B2', 'C1']]\n",
    "target = final_df['level']\n",
    "\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features, target, test_size=0.25, random_state=12345, shuffle=True)\n",
    "\n",
    "# размеры выборок\n",
    "print(f'Обучающая выборка:', features_train.shape[0])\n",
    "print(f'Валидационная выборка:', features_valid.shape[0])\n",
    "target_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb8aafdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sub_for_ml</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>amazing woman ever met think rachel elizabeth ...</td>\n",
       "      <td>0.738726</td>\n",
       "      <td>-0.567726</td>\n",
       "      <td>-1.118052</td>\n",
       "      <td>-0.744066</td>\n",
       "      <td>0.318372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>suing entire firm fraud never saw thing life d...</td>\n",
       "      <td>-0.043931</td>\n",
       "      <td>-0.667788</td>\n",
       "      <td>-0.431354</td>\n",
       "      <td>0.686724</td>\n",
       "      <td>1.049768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>paramount news brings special coverage princes...</td>\n",
       "      <td>1.296070</td>\n",
       "      <td>0.681081</td>\n",
       "      <td>1.345819</td>\n",
       "      <td>-1.049212</td>\n",
       "      <td>0.500117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>previously suits guilty bribing foreign govern...</td>\n",
       "      <td>-0.019008</td>\n",
       "      <td>-0.545521</td>\n",
       "      <td>0.391896</td>\n",
       "      <td>-0.044058</td>\n",
       "      <td>0.588139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>come put table okay go yeah take wrap hey hell...</td>\n",
       "      <td>1.967389</td>\n",
       "      <td>-0.313581</td>\n",
       "      <td>-1.045802</td>\n",
       "      <td>-1.910925</td>\n",
       "      <td>0.314997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sub_for_ml        A1        A2  \\\n",
       "71   amazing woman ever met think rachel elizabeth ...  0.738726 -0.567726   \n",
       "104  suing entire firm fraud never saw thing life d... -0.043931 -0.667788   \n",
       "68   paramount news brings special coverage princes...  1.296070  0.681081   \n",
       "154  previously suits guilty bribing foreign govern... -0.019008 -0.545521   \n",
       "120  come put table okay go yeah take wrap hey hell...  1.967389 -0.313581   \n",
       "\n",
       "           B1        B2        C1  \n",
       "71  -1.118052 -0.744066  0.318372  \n",
       "104 -0.431354  0.686724  1.049768  \n",
       "68   1.345819 -1.049212  0.500117  \n",
       "154  0.391896 -0.044058  0.588139  \n",
       "120 -1.045802 -1.910925  0.314997  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train_vec = features_train.copy()\n",
    "features_valid_vec = features_valid.copy()\n",
    "# Масштабируем количественные признаки и применим векторизацию к тексту:\n",
    "numeric = ['A1', 'A2', 'B1', 'B2', 'C1']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train_vec[numeric])\n",
    "\n",
    "features_train_vec[numeric] = scaler.transform(features_train_vec[numeric])\n",
    "features_valid_vec[numeric] = scaler.transform(features_valid_vec[numeric])\n",
    "\n",
    "\n",
    "# посмотрим, что получилось\n",
    "features_train_vec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aecca709",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "column_transformer = ColumnTransformer([('vect1', tfidf_vectorizer, 'sub_for_ml')], remainder='passthrough')\n",
    "\n",
    "features_train_vec = column_transformer.fit_transform(features_train_vec)\n",
    "features_valid_vec = column_transformer.transform(features_valid_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0efe22d",
   "metadata": {},
   "source": [
    "**ОБУЧЕНИЕ МОДЕЛИ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf4cadc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8426817881624195\n"
     ]
    }
   ],
   "source": [
    "model_LR = LogisticRegression(n_jobs=3,C=1e5, solver='saga', \n",
    "                                           multi_class='multinomial',\n",
    "                                           max_iter=1000,\n",
    "                                           random_state=42)\n",
    "model_LR.fit(features_train_vec, target_train)\n",
    "pred = model_LR .predict(features_valid_vec)\n",
    "print(f\"F1 Score: {f1_score(target_valid, pred, average='weighted')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a3d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611fadf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2bdb650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "def fit_model(train_pool, test_pool, **kwargs):\n",
    "    model = CatBoostClassifier(task_type='CPU', iterations = 5000,\n",
    "                               eval_metric='TotalF1', od_type='Iter', \n",
    "                               od_wait=500, **kwargs)\n",
    "    \n",
    "    return model.fit(train_pool, eval_set=test_pool, \n",
    "                     verbose=100, plot=True, \n",
    "                     use_best_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c8aefc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(data=features_train, label=target_train, \n",
    "                  text_features=['sub_for_ml'])\n",
    "valid_pool = Pool(data=features_valid, label=target_valid, \n",
    "                  text_features=['sub_for_ml'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9d54f8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f4a9eef0f141fcb78243eefd63fce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7705820\ttest: 0.7264808\tbest: 0.7264808 (0)\ttotal: 1.48s\tremaining: 2h 3m 29s\n",
      "100:\tlearn: 1.0000000\ttest: 0.8387050\tbest: 0.8387050 (50)\ttotal: 1m 31s\tremaining: 1h 13m 59s\n",
      "200:\tlearn: 1.0000000\ttest: 0.8387050\tbest: 0.8387050 (50)\ttotal: 3m 13s\tremaining: 1h 16m 55s\n",
      "300:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 4m 58s\tremaining: 1h 17m 36s\n",
      "400:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 6m 35s\tremaining: 1h 15m 40s\n",
      "500:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 8m 12s\tremaining: 1h 13m 44s\n",
      "600:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 9m 58s\tremaining: 1h 13m 3s\n",
      "700:\tlearn: 1.0000000\ttest: 0.8648638\tbest: 0.8648638 (204)\ttotal: 11m 41s\tremaining: 1h 11m 40s\n",
      "Stopped by overfitting detector  (500 iterations wait)\n",
      "\n",
      "bestTest = 0.8648637947\n",
      "bestIteration = 204\n",
      "\n",
      "Shrink model to first 205 iterations.\n"
     ]
    }
   ],
   "source": [
    "model = fit_model(train_pool, valid_pool, learning_rate=0.35,\n",
    "                  dictionaries = [{\n",
    "                      'dictionary_id':'Word',\n",
    "                      'max_dictionary_size': '50000'\n",
    "                  }],\n",
    "                 feature_calcers = ['BoW:top_tokens_count=10000'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151806e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d0c1f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Сохраним данную модель с наилучшими параметрами для дальнейшего использования:\n",
    "import pickle\n",
    "from pickle import dump,load\n",
    "with open(\"D:/jupiter_projects/мастерская_2/model_finish\",\"wb\") as fid: \n",
    "    dump(model,fid)\n",
    "with open(\"D:/jupiter_projects/мастерская_2/scaler\",\"wb\") as fid_1: \n",
    "    dump(scaler,fid_1)\n",
    "with open(\"D:/jupiter_projects/мастерская_2/column_transformer\",\"wb\") as fid_1: \n",
    "    dump(column_transformer,fid_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b5fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
